{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'C:\\\\Users\\\\hiremath\\\\Downloads\\\\devanagari\\\\Images\\\\vc'\n",
    "VAL_DIR = 'C:\\\\Users\\\\hiremath\\\\Downloads\\\\devanagari\\\\Images\\\\t'\n",
    "IMG_SIZE = 32\n",
    "LR = 0.001\n",
    "MODEL_NAME = 'devanagari-{}-{}.model'.format(LR,'5conv-basic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_img(pic):\n",
    "    l = pic.split('_')[-2]\n",
    "    a = []\n",
    "    a.insert(0,int(l))\n",
    "    label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
    "    label_binarizer.fit(range(46))\n",
    "    b = label_binarizer.transform(a)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    training_data = []\n",
    "    for pic in os.listdir(TRAIN_DIR):\n",
    "        DIR = os.path.join(TRAIN_DIR,pic)\n",
    "        for img in os.listdir(DIR):\n",
    "            if img != \"Thumbs - Copy.db\" and img != \"Thumbs.db\":\n",
    "                label = label_img(pic)\n",
    "                path1 = os.path.join(DIR,img)\n",
    "                with Image.open(path1).convert('L') as img1:\n",
    "                    img1 = img1.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n",
    "                    training_data.append([np.array(img1),np.array(label)])\n",
    "    \n",
    "    \n",
    "    shuffle(training_data)\n",
    "    np.save('train_dev.npy',training_data)\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_validation_data():\n",
    "    validation_data = []\n",
    "    for pic in os.listdir(VAL_DIR):\n",
    "        DIR = os.path.join(VAL_DIR,pic)\n",
    "        for img in os.listdir(DIR):\n",
    "            if img != \"Thumbs - Copy.db\" and img != \"Thumbs.db\":\n",
    "                label = label_img(pic)\n",
    "                path1 = os.path.join(DIR,img)\n",
    "                with Image.open(path1).convert('L') as img1:\n",
    "                    img1 = img1.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n",
    "                    validation_data.append([np.array(img1),np.array(label)])\n",
    "    \n",
    "    \n",
    "    shuffle(validation_data)\n",
    "    np.save('validation_dev.npy',validation_data)\n",
    "    return validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = create_train_data()\n",
    "#train_data = np.load('train_dev.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36846\n",
      "C:\\Users\\hiremath\\Downloads\\devanagari\\Images\\t\\character_46_9\\56777.png\n"
     ]
    }
   ],
   "source": [
    "val_data = create_validation_data()\n",
    "#val_data = np.load('validation_dev.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 128, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 46, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tflearn.DNN(convnet, tensorboard_dir='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists('{}.meta'.format(MODEL_NAME)):\n",
    "    model.load(MODEL_NAME)\n",
    "    print('model loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_data\n",
    "val = val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "Y = [i[1][0] for i in train]\n",
    "test_x = np.array([i[0] for i in val]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "test_y = [i[1][0] for i in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: devanagari-0.001-5conv-basic.model\n",
      "Log directory: log/\n",
      "---------------------------------\n",
      "Training samples: 55154\n",
      "Validation samples: 36846\n",
      "--\n",
      "Training Step: 2587  | total loss: \u001b[1m\u001b[32m0.14097\u001b[0m\u001b[0m | time: 37.459s\n",
      "| Adam | epoch: 007 | loss: 0.14097 - acc: 0.9532 | val_loss: 0.52162 - val_acc: 0.8442 -- iter: 00128/55154\n",
      "--\n",
      "Training Step: 2588  | total loss: \u001b[1m\u001b[32m0.13054\u001b[0m\u001b[0m | time: 71.925s\n",
      "| Adam | epoch: 007 | loss: 0.13054 - acc: 0.9540 | val_loss: 0.53037 - val_acc: 0.8424 -- iter: 00256/55154\n",
      "--\n",
      "Training Step: 2589  | total loss: \u001b[1m\u001b[32m0.12120\u001b[0m\u001b[0m | time: 115.864s\n",
      "| Adam | epoch: 007 | loss: 0.12120 - acc: 0.9539 | val_loss: 0.52387 - val_acc: 0.8438 -- iter: 00384/55154\n",
      "--\n",
      "Training Step: 2590  | total loss: \u001b[1m\u001b[32m0.11425\u001b[0m\u001b[0m | time: 159.730s\n",
      "| Adam | epoch: 007 | loss: 0.11425 - acc: 0.9515 | val_loss: 0.51390 - val_acc: 0.8460 -- iter: 00512/55154\n",
      "--\n",
      "Training Step: 2591  | total loss: \u001b[1m\u001b[32m0.11474\u001b[0m\u001b[0m | time: 207.964s\n",
      "| Adam | epoch: 007 | loss: 0.11474 - acc: 0.9524 | val_loss: 0.50900 - val_acc: 0.8458 -- iter: 00640/55154\n",
      "--\n",
      "Training Step: 2592  | total loss: \u001b[1m\u001b[32m0.11160\u001b[0m\u001b[0m | time: 247.335s\n",
      "| Adam | epoch: 007 | loss: 0.11160 - acc: 0.9525 | val_loss: 0.50756 - val_acc: 0.8470 -- iter: 00768/55154\n",
      "--\n",
      "Training Step: 2593  | total loss: \u001b[1m\u001b[32m0.10755\u001b[0m\u001b[0m | time: 290.878s\n",
      "| Adam | epoch: 007 | loss: 0.10755 - acc: 0.9502 | val_loss: 0.49958 - val_acc: 0.8486 -- iter: 00896/55154\n",
      "--\n",
      "Training Step: 2594  | total loss: \u001b[1m\u001b[32m0.10634\u001b[0m\u001b[0m | time: 332.269s\n",
      "| Adam | epoch: 007 | loss: 0.10634 - acc: 0.9521 | val_loss: 0.49270 - val_acc: 0.8494 -- iter: 01024/55154\n",
      "--\n",
      "Training Step: 2595  | total loss: \u001b[1m\u001b[32m0.10286\u001b[0m\u001b[0m | time: 371.755s\n",
      "| Adam | epoch: 007 | loss: 0.10286 - acc: 0.9537 | val_loss: 0.48508 - val_acc: 0.8522 -- iter: 01152/55154\n",
      "--\n",
      "Training Step: 2596  | total loss: \u001b[1m\u001b[32m0.09612\u001b[0m\u001b[0m | time: 414.210s\n",
      "| Adam | epoch: 007 | loss: 0.09612 - acc: 0.9560 | val_loss: 0.48263 - val_acc: 0.8535 -- iter: 01280/55154\n",
      "--\n",
      "Training Step: 2597  | total loss: \u001b[1m\u001b[32m0.09015\u001b[0m\u001b[0m | time: 456.805s\n",
      "| Adam | epoch: 007 | loss: 0.09015 - acc: 0.9557 | val_loss: 0.49331 - val_acc: 0.8515 -- iter: 01408/55154\n",
      "--\n",
      "Training Step: 2598  | total loss: \u001b[1m\u001b[32m0.08533\u001b[0m\u001b[0m | time: 496.993s\n",
      "| Adam | epoch: 007 | loss: 0.08533 - acc: 0.9578 | val_loss: 0.51194 - val_acc: 0.8469 -- iter: 01536/55154\n",
      "--\n",
      "Training Step: 2599  | total loss: \u001b[1m\u001b[32m0.08618\u001b[0m\u001b[0m | time: 540.100s\n",
      "| Adam | epoch: 007 | loss: 0.08618 - acc: 0.9566 | val_loss: 0.53967 - val_acc: 0.8420 -- iter: 01664/55154\n",
      "--\n",
      "Training Step: 2600  | total loss: \u001b[1m\u001b[32m0.08165\u001b[0m\u001b[0m | time: 578.640s\n",
      "| Adam | epoch: 007 | loss: 0.08165 - acc: 0.9570 | val_loss: 0.53528 - val_acc: 0.8433 -- iter: 01792/55154\n",
      "--\n",
      "Training Step: 2601  | total loss: \u001b[1m\u001b[32m0.07975\u001b[0m\u001b[0m | time: 615.106s\n",
      "| Adam | epoch: 007 | loss: 0.07975 - acc: 0.9543 | val_loss: 0.53108 - val_acc: 0.8448 -- iter: 01920/55154\n",
      "--\n",
      "Training Step: 2602  | total loss: \u001b[1m\u001b[32m0.07562\u001b[0m\u001b[0m | time: 650.039s\n",
      "| Adam | epoch: 007 | loss: 0.07562 - acc: 0.9557 | val_loss: 0.54033 - val_acc: 0.8436 -- iter: 02048/55154\n",
      "--\n",
      "Training Step: 2603  | total loss: \u001b[1m\u001b[32m0.07634\u001b[0m\u001b[0m | time: 684.760s\n",
      "| Adam | epoch: 007 | loss: 0.07634 - acc: 0.9555 | val_loss: 0.54690 - val_acc: 0.8434 -- iter: 02176/55154\n",
      "--\n",
      "Training Step: 2604  | total loss: \u001b[1m\u001b[32m0.07287\u001b[0m\u001b[0m | time: 720.968s\n",
      "| Adam | epoch: 007 | loss: 0.07287 - acc: 0.9568 | val_loss: 0.54750 - val_acc: 0.8445 -- iter: 02304/55154\n",
      "--\n",
      "Training Step: 2605  | total loss: \u001b[1m\u001b[32m0.07527\u001b[0m\u001b[0m | time: 762.843s\n",
      "| Adam | epoch: 007 | loss: 0.07527 - acc: 0.9572 | val_loss: 0.54499 - val_acc: 0.8455 -- iter: 02432/55154\n",
      "--\n",
      "Training Step: 2606  | total loss: \u001b[1m\u001b[32m0.07412\u001b[0m\u001b[0m | time: 800.938s\n",
      "| Adam | epoch: 007 | loss: 0.07412 - acc: 0.9584 | val_loss: 0.54104 - val_acc: 0.8466 -- iter: 02560/55154\n",
      "--\n",
      "Training Step: 2607  | total loss: \u001b[1m\u001b[32m0.07337\u001b[0m\u001b[0m | time: 843.054s\n",
      "| Adam | epoch: 007 | loss: 0.07337 - acc: 0.9578 | val_loss: 0.53771 - val_acc: 0.8477 -- iter: 02688/55154\n",
      "--\n",
      "Training Step: 2608  | total loss: \u001b[1m\u001b[32m0.07201\u001b[0m\u001b[0m | time: 882.448s\n",
      "| Adam | epoch: 007 | loss: 0.07201 - acc: 0.9566 | val_loss: 0.54368 - val_acc: 0.8471 -- iter: 02816/55154\n",
      "--\n",
      "Training Step: 2609  | total loss: \u001b[1m\u001b[32m0.06562\u001b[0m\u001b[0m | time: 923.929s\n",
      "| Adam | epoch: 007 | loss: 0.06562 - acc: 0.9601 | val_loss: 0.53757 - val_acc: 0.8475 -- iter: 02944/55154\n",
      "--\n",
      "Training Step: 2610  | total loss: \u001b[1m\u001b[32m0.06453\u001b[0m\u001b[0m | time: 965.265s\n",
      "| Adam | epoch: 007 | loss: 0.06453 - acc: 0.9633 | val_loss: 0.53705 - val_acc: 0.8472 -- iter: 03072/55154\n",
      "--\n",
      "Training Step: 2611  | total loss: \u001b[1m\u001b[32m0.06416\u001b[0m\u001b[0m | time: 1008.632s\n",
      "| Adam | epoch: 007 | loss: 0.06416 - acc: 0.9647 | val_loss: 0.53120 - val_acc: 0.8471 -- iter: 03200/55154\n",
      "--\n",
      "Training Step: 2612  | total loss: \u001b[1m\u001b[32m0.06264\u001b[0m\u001b[0m | time: 1049.830s\n",
      "| Adam | epoch: 007 | loss: 0.06264 - acc: 0.9643 | val_loss: 0.53615 - val_acc: 0.8450 -- iter: 03328/55154\n",
      "--\n",
      "Training Step: 2613  | total loss: \u001b[1m\u001b[32m0.06328\u001b[0m\u001b[0m | time: 1089.697s\n",
      "| Adam | epoch: 007 | loss: 0.06328 - acc: 0.9647 | val_loss: 0.54317 - val_acc: 0.8437 -- iter: 03456/55154\n",
      "--\n",
      "Training Step: 2614  | total loss: \u001b[1m\u001b[32m0.06288\u001b[0m\u001b[0m | time: 1125.966s\n",
      "| Adam | epoch: 007 | loss: 0.06288 - acc: 0.9659 | val_loss: 0.52247 - val_acc: 0.8471 -- iter: 03584/55154\n",
      "--\n",
      "Training Step: 2615  | total loss: \u001b[1m\u001b[32m0.06394\u001b[0m\u001b[0m | time: 1163.807s\n",
      "| Adam | epoch: 007 | loss: 0.06394 - acc: 0.9654 | val_loss: 0.50885 - val_acc: 0.8494 -- iter: 03712/55154\n",
      "--\n",
      "Training Step: 2616  | total loss: \u001b[1m\u001b[32m0.06650\u001b[0m\u001b[0m | time: 1202.629s\n",
      "| Adam | epoch: 007 | loss: 0.06650 - acc: 0.9658 | val_loss: 0.48027 - val_acc: 0.8547 -- iter: 03840/55154\n",
      "--\n",
      "Training Step: 2617  | total loss: \u001b[1m\u001b[32m0.06873\u001b[0m\u001b[0m | time: 1240.969s\n",
      "| Adam | epoch: 007 | loss: 0.06873 - acc: 0.9645 | val_loss: 0.48534 - val_acc: 0.8539 -- iter: 03968/55154\n",
      "--\n",
      "Training Step: 2618  | total loss: \u001b[1m\u001b[32m0.06460\u001b[0m\u001b[0m | time: 1279.832s\n",
      "| Adam | epoch: 007 | loss: 0.06460 - acc: 0.9657 | val_loss: 0.51374 - val_acc: 0.8470 -- iter: 04096/55154\n",
      "--\n",
      "Training Step: 2619  | total loss: \u001b[1m\u001b[32m0.05992\u001b[0m\u001b[0m | time: 1318.136s\n",
      "| Adam | epoch: 007 | loss: 0.05992 - acc: 0.9637 | val_loss: 0.54625 - val_acc: 0.8422 -- iter: 04224/55154\n",
      "--\n",
      "Training Step: 2620  | total loss: \u001b[1m\u001b[32m0.05893\u001b[0m\u001b[0m | time: 1358.160s\n",
      "| Adam | epoch: 007 | loss: 0.05893 - acc: 0.9610 | val_loss: 0.54732 - val_acc: 0.8422 -- iter: 04352/55154\n",
      "--\n",
      "Training Step: 2621  | total loss: \u001b[1m\u001b[32m0.07019\u001b[0m\u001b[0m | time: 1396.463s\n",
      "| Adam | epoch: 007 | loss: 0.07019 - acc: 0.9595 | val_loss: 0.52621 - val_acc: 0.8482 -- iter: 04480/55154\n",
      "--\n",
      "Training Step: 2622  | total loss: \u001b[1m\u001b[32m0.08495\u001b[0m\u001b[0m | time: 1435.688s\n",
      "| Adam | epoch: 007 | loss: 0.08495 - acc: 0.9573 | val_loss: 0.51612 - val_acc: 0.8508 -- iter: 04608/55154\n",
      "--\n",
      "Training Step: 2623  | total loss: \u001b[1m\u001b[32m0.09171\u001b[0m\u001b[0m | time: 1476.321s\n",
      "| Adam | epoch: 007 | loss: 0.09171 - acc: 0.9561 | val_loss: 0.53114 - val_acc: 0.8486 -- iter: 04736/55154\n",
      "--\n",
      "Training Step: 2624  | total loss: \u001b[1m\u001b[32m0.08679\u001b[0m\u001b[0m | time: 1511.915s\n",
      "| Adam | epoch: 007 | loss: 0.08679 - acc: 0.9558 | val_loss: 0.56123 - val_acc: 0.8407 -- iter: 04864/55154\n",
      "--\n",
      "Training Step: 2625  | total loss: \u001b[1m\u001b[32m0.08837\u001b[0m\u001b[0m | time: 1547.352s\n",
      "| Adam | epoch: 007 | loss: 0.08837 - acc: 0.9547 | val_loss: 0.58018 - val_acc: 0.8360 -- iter: 04992/55154\n",
      "--\n",
      "Training Step: 2626  | total loss: \u001b[1m\u001b[32m0.08516\u001b[0m\u001b[0m | time: 1593.220s\n",
      "| Adam | epoch: 007 | loss: 0.08516 - acc: 0.9561 | val_loss: 0.56990 - val_acc: 0.8365 -- iter: 05120/55154\n",
      "--\n",
      "Training Step: 2627  | total loss: \u001b[1m\u001b[32m0.08189\u001b[0m\u001b[0m | time: 1629.713s\n",
      "| Adam | epoch: 007 | loss: 0.08189 - acc: 0.9535 | val_loss: 0.56500 - val_acc: 0.8367 -- iter: 05248/55154\n",
      "--\n",
      "Training Step: 2628  | total loss: \u001b[1m\u001b[32m0.07727\u001b[0m\u001b[0m | time: 1666.350s\n",
      "| Adam | epoch: 007 | loss: 0.07727 - acc: 0.9542 | val_loss: 0.55764 - val_acc: 0.8376 -- iter: 05376/55154\n",
      "--\n",
      "Training Step: 2629  | total loss: \u001b[1m\u001b[32m0.07602\u001b[0m\u001b[0m | time: 1702.913s\n",
      "| Adam | epoch: 007 | loss: 0.07602 - acc: 0.9549 | val_loss: 0.55328 - val_acc: 0.8395 -- iter: 05504/55154\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2630  | total loss: \u001b[1m\u001b[32m0.07068\u001b[0m\u001b[0m | time: 1739.660s\n",
      "| Adam | epoch: 007 | loss: 0.07068 - acc: 0.9579 | val_loss: 0.54675 - val_acc: 0.8421 -- iter: 05632/55154\n",
      "--\n",
      "Training Step: 2631  | total loss: \u001b[1m\u001b[32m0.06655\u001b[0m\u001b[0m | time: 1778.885s\n",
      "| Adam | epoch: 007 | loss: 0.06655 - acc: 0.9605 | val_loss: 0.52971 - val_acc: 0.8450 -- iter: 05760/55154\n",
      "--\n",
      "Training Step: 2632  | total loss: \u001b[1m\u001b[32m0.06457\u001b[0m\u001b[0m | time: 1817.736s\n",
      "| Adam | epoch: 007 | loss: 0.06457 - acc: 0.9598 | val_loss: 0.50795 - val_acc: 0.8496 -- iter: 05888/55154\n",
      "--\n",
      "Training Step: 2633  | total loss: \u001b[1m\u001b[32m0.06606\u001b[0m\u001b[0m | time: 1858.318s\n",
      "| Adam | epoch: 007 | loss: 0.06606 - acc: 0.9583 | val_loss: 0.47887 - val_acc: 0.8545 -- iter: 06016/55154\n",
      "--\n",
      "Training Step: 2634  | total loss: \u001b[1m\u001b[32m0.06830\u001b[0m\u001b[0m | time: 1892.043s\n",
      "| Adam | epoch: 007 | loss: 0.06830 - acc: 0.9570 | val_loss: 0.45883 - val_acc: 0.8570 -- iter: 06144/55154\n",
      "--\n",
      "Training Step: 2635  | total loss: \u001b[1m\u001b[32m0.06682\u001b[0m\u001b[0m | time: 1926.060s\n",
      "| Adam | epoch: 007 | loss: 0.06682 - acc: 0.9551 | val_loss: 0.45556 - val_acc: 0.8560 -- iter: 06272/55154\n",
      "--\n",
      "Training Step: 2636  | total loss: \u001b[1m\u001b[32m0.06419\u001b[0m\u001b[0m | time: 1959.973s\n",
      "| Adam | epoch: 007 | loss: 0.06419 - acc: 0.9564 | val_loss: 0.47031 - val_acc: 0.8519 -- iter: 06400/55154\n",
      "--\n",
      "Training Step: 2637  | total loss: \u001b[1m\u001b[32m0.06096\u001b[0m\u001b[0m | time: 1993.755s\n",
      "| Adam | epoch: 007 | loss: 0.06096 - acc: 0.9592 | val_loss: 0.48635 - val_acc: 0.8483 -- iter: 06528/55154\n",
      "--\n",
      "Training Step: 2638  | total loss: \u001b[1m\u001b[32m0.05815\u001b[0m\u001b[0m | time: 2027.727s\n",
      "| Adam | epoch: 007 | loss: 0.05815 - acc: 0.9602 | val_loss: 0.50430 - val_acc: 0.8453 -- iter: 06656/55154\n",
      "--\n",
      "Training Step: 2639  | total loss: \u001b[1m\u001b[32m0.05745\u001b[0m\u001b[0m | time: 2061.577s\n",
      "| Adam | epoch: 007 | loss: 0.05745 - acc: 0.9610 | val_loss: 0.52494 - val_acc: 0.8417 -- iter: 06784/55154\n",
      "--\n",
      "Training Step: 2640  | total loss: \u001b[1m\u001b[32m0.06022\u001b[0m\u001b[0m | time: 2100.334s\n",
      "| Adam | epoch: 007 | loss: 0.06022 - acc: 0.9595 | val_loss: 0.54851 - val_acc: 0.8392 -- iter: 06912/55154\n",
      "--\n",
      "Training Step: 2641  | total loss: \u001b[1m\u001b[32m0.05736\u001b[0m\u001b[0m | time: 2134.153s\n",
      "| Adam | epoch: 007 | loss: 0.05736 - acc: 0.9604 | val_loss: 0.55926 - val_acc: 0.8379 -- iter: 07040/55154\n",
      "--\n",
      "Training Step: 2642  | total loss: \u001b[1m\u001b[32m0.06707\u001b[0m\u001b[0m | time: 2167.838s\n",
      "| Adam | epoch: 007 | loss: 0.06707 - acc: 0.9542 | val_loss: 0.55935 - val_acc: 0.8384 -- iter: 07168/55154\n",
      "--\n",
      "Training Step: 2643  | total loss: \u001b[1m\u001b[32m0.07318\u001b[0m\u001b[0m | time: 2201.818s\n",
      "| Adam | epoch: 007 | loss: 0.07318 - acc: 0.9533 | val_loss: 0.54884 - val_acc: 0.8395 -- iter: 07296/55154\n",
      "--\n",
      "Training Step: 2644  | total loss: \u001b[1m\u001b[32m0.07420\u001b[0m\u001b[0m | time: 2235.555s\n",
      "| Adam | epoch: 007 | loss: 0.07420 - acc: 0.9541 | val_loss: 0.52368 - val_acc: 0.8448 -- iter: 07424/55154\n",
      "--\n",
      "Training Step: 2645  | total loss: \u001b[1m\u001b[32m0.07518\u001b[0m\u001b[0m | time: 2269.435s\n",
      "| Adam | epoch: 007 | loss: 0.07518 - acc: 0.9532 | val_loss: 0.49851 - val_acc: 0.8510 -- iter: 07552/55154\n",
      "--\n",
      "Training Step: 2646  | total loss: \u001b[1m\u001b[32m0.08083\u001b[0m\u001b[0m | time: 2303.523s\n",
      "| Adam | epoch: 007 | loss: 0.08083 - acc: 0.9548 | val_loss: 0.49413 - val_acc: 0.8535 -- iter: 07680/55154\n",
      "--\n",
      "Training Step: 2647  | total loss: \u001b[1m\u001b[32m0.08185\u001b[0m\u001b[0m | time: 2337.640s\n",
      "| Adam | epoch: 007 | loss: 0.08185 - acc: 0.9530 | val_loss: 0.51482 - val_acc: 0.8492 -- iter: 07808/55154\n",
      "--\n",
      "Training Step: 2648  | total loss: \u001b[1m\u001b[32m0.07492\u001b[0m\u001b[0m | time: 2371.880s\n",
      "| Adam | epoch: 007 | loss: 0.07492 - acc: 0.9546 | val_loss: 0.53596 - val_acc: 0.8449 -- iter: 07936/55154\n",
      "--\n",
      "Training Step: 2649  | total loss: \u001b[1m\u001b[32m0.07041\u001b[0m\u001b[0m | time: 2405.942s\n",
      "| Adam | epoch: 007 | loss: 0.07041 - acc: 0.9545 | val_loss: 0.54334 - val_acc: 0.8432 -- iter: 08064/55154\n",
      "--\n",
      "Training Step: 2650  | total loss: \u001b[1m\u001b[32m0.07618\u001b[0m\u001b[0m | time: 2448.214s\n",
      "| Adam | epoch: 007 | loss: 0.07618 - acc: 0.9528 | val_loss: 0.54400 - val_acc: 0.8443 -- iter: 08192/55154\n",
      "--\n",
      "Training Step: 2651  | total loss: \u001b[1m\u001b[32m0.07612\u001b[0m\u001b[0m | time: 2487.776s\n",
      "| Adam | epoch: 007 | loss: 0.07612 - acc: 0.9528 | val_loss: 0.51922 - val_acc: 0.8498 -- iter: 08320/55154\n",
      "--\n",
      "Training Step: 2652  | total loss: \u001b[1m\u001b[32m0.07842\u001b[0m\u001b[0m | time: 2524.595s\n",
      "| Adam | epoch: 007 | loss: 0.07842 - acc: 0.9528 | val_loss: 0.50561 - val_acc: 0.8542 -- iter: 08448/55154\n",
      "--\n",
      "Training Step: 2653  | total loss: \u001b[1m\u001b[32m0.07662\u001b[0m\u001b[0m | time: 2565.838s\n",
      "| Adam | epoch: 007 | loss: 0.07662 - acc: 0.9544 | val_loss: 0.50935 - val_acc: 0.8535 -- iter: 08576/55154\n",
      "--\n",
      "Training Step: 2654  | total loss: \u001b[1m\u001b[32m0.07391\u001b[0m\u001b[0m | time: 2599.633s\n",
      "| Adam | epoch: 007 | loss: 0.07391 - acc: 0.9566 | val_loss: 0.51919 - val_acc: 0.8509 -- iter: 08704/55154\n",
      "--\n",
      "Training Step: 2655  | total loss: \u001b[1m\u001b[32m0.07070\u001b[0m\u001b[0m | time: 2637.546s\n",
      "| Adam | epoch: 007 | loss: 0.07070 - acc: 0.9571 | val_loss: 0.53691 - val_acc: 0.8470 -- iter: 08832/55154\n",
      "--\n",
      "Training Step: 2656  | total loss: \u001b[1m\u001b[32m0.06939\u001b[0m\u001b[0m | time: 2671.687s\n",
      "| Adam | epoch: 007 | loss: 0.06939 - acc: 0.9575 | val_loss: 0.55834 - val_acc: 0.8416 -- iter: 08960/55154\n",
      "--\n",
      "Training Step: 2657  | total loss: \u001b[1m\u001b[32m0.07084\u001b[0m\u001b[0m | time: 2705.423s\n",
      "| Adam | epoch: 007 | loss: 0.07084 - acc: 0.9547 | val_loss: 0.54069 - val_acc: 0.8426 -- iter: 09088/55154\n",
      "--\n",
      "Training Step: 2658  | total loss: \u001b[1m\u001b[32m0.07948\u001b[0m\u001b[0m | time: 2741.089s\n",
      "| Adam | epoch: 007 | loss: 0.07948 - acc: 0.9530 | val_loss: 0.50101 - val_acc: 0.8489 -- iter: 09216/55154\n",
      "--\n",
      "Training Step: 2659  | total loss: \u001b[1m\u001b[32m0.07843\u001b[0m\u001b[0m | time: 2779.045s\n",
      "| Adam | epoch: 007 | loss: 0.07843 - acc: 0.9530 | val_loss: 0.46522 - val_acc: 0.8565 -- iter: 09344/55154\n",
      "--\n",
      "Training Step: 2660  | total loss: \u001b[1m\u001b[32m0.07709\u001b[0m\u001b[0m | time: 2814.501s\n",
      "| Adam | epoch: 007 | loss: 0.07709 - acc: 0.9538 | val_loss: 0.45812 - val_acc: 0.8584 -- iter: 09472/55154\n",
      "--\n",
      "Training Step: 2661  | total loss: \u001b[1m\u001b[32m0.07549\u001b[0m\u001b[0m | time: 2850.695s\n",
      "| Adam | epoch: 007 | loss: 0.07549 - acc: 0.9568 | val_loss: 0.45937 - val_acc: 0.8578 -- iter: 09600/55154\n",
      "--\n",
      "Training Step: 2662  | total loss: \u001b[1m\u001b[32m0.07490\u001b[0m\u001b[0m | time: 2884.560s\n",
      "| Adam | epoch: 007 | loss: 0.07490 - acc: 0.9565 | val_loss: 0.45192 - val_acc: 0.8595 -- iter: 09728/55154\n",
      "--\n",
      "Training Step: 2663  | total loss: \u001b[1m\u001b[32m0.07351\u001b[0m\u001b[0m | time: 2918.236s\n",
      "| Adam | epoch: 007 | loss: 0.07351 - acc: 0.9538 | val_loss: 0.45004 - val_acc: 0.8610 -- iter: 09856/55154\n",
      "--\n",
      "Training Step: 2664  | total loss: \u001b[1m\u001b[32m0.06792\u001b[0m\u001b[0m | time: 2952.178s\n",
      "| Adam | epoch: 007 | loss: 0.06792 - acc: 0.9545 | val_loss: 0.44849 - val_acc: 0.8629 -- iter: 09984/55154\n",
      "--\n",
      "Training Step: 2665  | total loss: \u001b[1m\u001b[32m0.06735\u001b[0m\u001b[0m | time: 2986.240s\n",
      "| Adam | epoch: 007 | loss: 0.06735 - acc: 0.9559 | val_loss: 0.45889 - val_acc: 0.8612 -- iter: 10112/55154\n",
      "--\n",
      "Training Step: 2666  | total loss: \u001b[1m\u001b[32m0.07760\u001b[0m\u001b[0m | time: 3021.142s\n",
      "| Adam | epoch: 007 | loss: 0.07760 - acc: 0.9494 | val_loss: 0.47024 - val_acc: 0.8592 -- iter: 10240/55154\n",
      "--\n",
      "Training Step: 2667  | total loss: \u001b[1m\u001b[32m0.07460\u001b[0m\u001b[0m | time: 3062.377s\n",
      "| Adam | epoch: 007 | loss: 0.07460 - acc: 0.9505 | val_loss: 0.48850 - val_acc: 0.8551 -- iter: 10368/55154\n",
      "--\n",
      "Training Step: 2668  | total loss: \u001b[1m\u001b[32m0.07421\u001b[0m\u001b[0m | time: 3105.526s\n",
      "| Adam | epoch: 007 | loss: 0.07421 - acc: 0.9508 | val_loss: 0.50686 - val_acc: 0.8512 -- iter: 10496/55154\n",
      "--\n",
      "Training Step: 2669  | total loss: \u001b[1m\u001b[32m0.07274\u001b[0m\u001b[0m | time: 3144.641s\n",
      "| Adam | epoch: 007 | loss: 0.07274 - acc: 0.9503 | val_loss: 0.52590 - val_acc: 0.8470 -- iter: 10624/55154\n",
      "--\n",
      "Training Step: 2670  | total loss: \u001b[1m\u001b[32m0.08269\u001b[0m\u001b[0m | time: 3183.738s\n",
      "| Adam | epoch: 007 | loss: 0.08269 - acc: 0.9498 | val_loss: 0.49873 - val_acc: 0.8524 -- iter: 10752/55154\n",
      "--\n",
      "Training Step: 2671  | total loss: \u001b[1m\u001b[32m0.08929\u001b[0m\u001b[0m | time: 3224.093s\n",
      "| Adam | epoch: 007 | loss: 0.08929 - acc: 0.9454 | val_loss: 0.47777 - val_acc: 0.8559 -- iter: 10880/55154\n",
      "--\n",
      "Training Step: 2672  | total loss: \u001b[1m\u001b[32m0.08260\u001b[0m\u001b[0m | time: 3263.583s\n",
      "| Adam | epoch: 007 | loss: 0.08260 - acc: 0.9470 | val_loss: 0.45255 - val_acc: 0.8604 -- iter: 11008/55154\n",
      "--\n",
      "Training Step: 2673  | total loss: \u001b[1m\u001b[32m0.07751\u001b[0m\u001b[0m | time: 3302.525s\n",
      "| Adam | epoch: 007 | loss: 0.07751 - acc: 0.9484 | val_loss: 0.43646 - val_acc: 0.8635 -- iter: 11136/55154\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2674  | total loss: \u001b[1m\u001b[32m0.07302\u001b[0m\u001b[0m | time: 3341.760s\n",
      "| Adam | epoch: 007 | loss: 0.07302 - acc: 0.9473 | val_loss: 0.44578 - val_acc: 0.8601 -- iter: 11264/55154\n",
      "--\n",
      "Training Step: 2675  | total loss: \u001b[1m\u001b[32m0.06793\u001b[0m\u001b[0m | time: 3383.110s\n",
      "| Adam | epoch: 007 | loss: 0.06793 - acc: 0.9494 | val_loss: 0.46743 - val_acc: 0.8544 -- iter: 11392/55154\n",
      "--\n",
      "Training Step: 2676  | total loss: \u001b[1m\u001b[32m0.06846\u001b[0m\u001b[0m | time: 3420.671s\n",
      "| Adam | epoch: 007 | loss: 0.06846 - acc: 0.9474 | val_loss: 0.49960 - val_acc: 0.8483 -- iter: 11520/55154\n",
      "--\n",
      "Training Step: 2677  | total loss: \u001b[1m\u001b[32m0.06403\u001b[0m\u001b[0m | time: 3457.267s\n",
      "| Adam | epoch: 007 | loss: 0.06403 - acc: 0.9496 | val_loss: 0.51099 - val_acc: 0.8455 -- iter: 11648/55154\n",
      "--\n",
      "Training Step: 2678  | total loss: \u001b[1m\u001b[32m0.06594\u001b[0m\u001b[0m | time: 3494.248s\n",
      "| Adam | epoch: 007 | loss: 0.06594 - acc: 0.9492 | val_loss: 0.49827 - val_acc: 0.8476 -- iter: 11776/55154\n",
      "--\n",
      "Training Step: 2679  | total loss: \u001b[1m\u001b[32m0.06553\u001b[0m\u001b[0m | time: 3533.640s\n",
      "| Adam | epoch: 007 | loss: 0.06553 - acc: 0.9449 | val_loss: 0.48067 - val_acc: 0.8528 -- iter: 11904/55154\n",
      "--\n",
      "Training Step: 2680  | total loss: \u001b[1m\u001b[32m0.06957\u001b[0m\u001b[0m | time: 3570.981s\n",
      "| Adam | epoch: 007 | loss: 0.06957 - acc: 0.9441 | val_loss: 0.46783 - val_acc: 0.8558 -- iter: 12032/55154\n",
      "--\n",
      "Training Step: 2681  | total loss: \u001b[1m\u001b[32m0.07350\u001b[0m\u001b[0m | time: 3608.999s\n",
      "| Adam | epoch: 007 | loss: 0.07350 - acc: 0.9427 | val_loss: 0.46103 - val_acc: 0.8578 -- iter: 12160/55154\n",
      "--\n",
      "Training Step: 2682  | total loss: \u001b[1m\u001b[32m0.07469\u001b[0m\u001b[0m | time: 3646.290s\n",
      "| Adam | epoch: 007 | loss: 0.07469 - acc: 0.9453 | val_loss: 0.47277 - val_acc: 0.8550 -- iter: 12288/55154\n",
      "--\n",
      "Training Step: 2683  | total loss: \u001b[1m\u001b[32m0.07211\u001b[0m\u001b[0m | time: 3683.945s\n",
      "| Adam | epoch: 007 | loss: 0.07211 - acc: 0.9469 | val_loss: 0.48311 - val_acc: 0.8531 -- iter: 12416/55154\n",
      "--\n",
      "Training Step: 2684  | total loss: \u001b[1m\u001b[32m0.06779\u001b[0m\u001b[0m | time: 3725.195s\n",
      "| Adam | epoch: 007 | loss: 0.06779 - acc: 0.9506 | val_loss: 0.49463 - val_acc: 0.8505 -- iter: 12544/55154\n",
      "--\n",
      "Training Step: 2685  | total loss: \u001b[1m\u001b[32m0.06338\u001b[0m\u001b[0m | time: 3765.991s\n",
      "| Adam | epoch: 007 | loss: 0.06338 - acc: 0.9516 | val_loss: 0.49478 - val_acc: 0.8504 -- iter: 12672/55154\n",
      "--\n",
      "Training Step: 2686  | total loss: \u001b[1m\u001b[32m0.06144\u001b[0m\u001b[0m | time: 3807.835s\n",
      "| Adam | epoch: 007 | loss: 0.06144 - acc: 0.9526 | val_loss: 0.49728 - val_acc: 0.8495 -- iter: 12800/55154\n",
      "--\n",
      "Training Step: 2687  | total loss: \u001b[1m\u001b[32m0.06238\u001b[0m\u001b[0m | time: 3849.405s\n",
      "| Adam | epoch: 007 | loss: 0.06238 - acc: 0.9557 | val_loss: 0.48264 - val_acc: 0.8528 -- iter: 12928/55154\n",
      "--\n",
      "Training Step: 2688  | total loss: \u001b[1m\u001b[32m0.06775\u001b[0m\u001b[0m | time: 3890.861s\n",
      "| Adam | epoch: 007 | loss: 0.06775 - acc: 0.9539 | val_loss: 0.46679 - val_acc: 0.8584 -- iter: 13056/55154\n",
      "--\n",
      "Training Step: 2689  | total loss: \u001b[1m\u001b[32m0.06843\u001b[0m\u001b[0m | time: 3934.373s\n",
      "| Adam | epoch: 007 | loss: 0.06843 - acc: 0.9531 | val_loss: 0.47584 - val_acc: 0.8579 -- iter: 13184/55154\n",
      "--\n",
      "Training Step: 2690  | total loss: \u001b[1m\u001b[32m0.06517\u001b[0m\u001b[0m | time: 3977.638s\n",
      "| Adam | epoch: 007 | loss: 0.06517 - acc: 0.9554 | val_loss: 0.50086 - val_acc: 0.8547 -- iter: 13312/55154\n",
      "--\n",
      "Training Step: 2691  | total loss: \u001b[1m\u001b[32m0.06877\u001b[0m\u001b[0m | time: 4019.412s\n",
      "| Adam | epoch: 007 | loss: 0.06877 - acc: 0.9544 | val_loss: 0.56335 - val_acc: 0.8450 -- iter: 13440/55154\n",
      "--\n",
      "Training Step: 2692  | total loss: \u001b[1m\u001b[32m0.07107\u001b[0m\u001b[0m | time: 4060.945s\n",
      "| Adam | epoch: 007 | loss: 0.07107 - acc: 0.9543 | val_loss: 0.62153 - val_acc: 0.8337 -- iter: 13568/55154\n",
      "--\n",
      "Training Step: 2693  | total loss: \u001b[1m\u001b[32m0.06854\u001b[0m\u001b[0m | time: 4103.543s\n",
      "| Adam | epoch: 007 | loss: 0.06854 - acc: 0.9565 | val_loss: 0.66341 - val_acc: 0.8256 -- iter: 13696/55154\n",
      "--\n",
      "Training Step: 2694  | total loss: \u001b[1m\u001b[32m0.07088\u001b[0m\u001b[0m | time: 4142.737s\n",
      "| Adam | epoch: 007 | loss: 0.07088 - acc: 0.9569 | val_loss: 0.62117 - val_acc: 0.8338 -- iter: 13824/55154\n",
      "--\n",
      "Training Step: 2695  | total loss: \u001b[1m\u001b[32m0.07264\u001b[0m\u001b[0m | time: 4179.603s\n",
      "| Adam | epoch: 007 | loss: 0.07264 - acc: 0.9573 | val_loss: 0.58164 - val_acc: 0.8415 -- iter: 13952/55154\n",
      "--\n",
      "Training Step: 2696  | total loss: \u001b[1m\u001b[32m0.07448\u001b[0m\u001b[0m | time: 4216.690s\n",
      "| Adam | epoch: 007 | loss: 0.07448 - acc: 0.9577 | val_loss: 0.55367 - val_acc: 0.8467 -- iter: 14080/55154\n",
      "--\n",
      "Training Step: 2697  | total loss: \u001b[1m\u001b[32m0.07937\u001b[0m\u001b[0m | time: 4253.490s\n",
      "| Adam | epoch: 007 | loss: 0.07937 - acc: 0.9557 | val_loss: 0.54641 - val_acc: 0.8489 -- iter: 14208/55154\n",
      "--\n",
      "Training Step: 2698  | total loss: \u001b[1m\u001b[32m0.07815\u001b[0m\u001b[0m | time: 4290.438s\n",
      "| Adam | epoch: 007 | loss: 0.07815 - acc: 0.9546 | val_loss: 0.55481 - val_acc: 0.8470 -- iter: 14336/55154\n",
      "--\n",
      "Training Step: 2699  | total loss: \u001b[1m\u001b[32m0.07519\u001b[0m\u001b[0m | time: 4327.092s\n",
      "| Adam | epoch: 007 | loss: 0.07519 - acc: 0.9553 | val_loss: 0.57698 - val_acc: 0.8424 -- iter: 14464/55154\n",
      "--\n",
      "Training Step: 2700  | total loss: \u001b[1m\u001b[32m0.06974\u001b[0m\u001b[0m | time: 4363.945s\n",
      "| Adam | epoch: 007 | loss: 0.06974 - acc: 0.9558 | val_loss: 0.61653 - val_acc: 0.8355 -- iter: 14592/55154\n",
      "--\n",
      "Training Step: 2701  | total loss: \u001b[1m\u001b[32m0.07053\u001b[0m\u001b[0m | time: 4400.931s\n",
      "| Adam | epoch: 007 | loss: 0.07053 - acc: 0.9556 | val_loss: 0.64542 - val_acc: 0.8296 -- iter: 14720/55154\n",
      "--\n",
      "Training Step: 2702  | total loss: \u001b[1m\u001b[32m0.07753\u001b[0m\u001b[0m | time: 4438.547s\n",
      "| Adam | epoch: 007 | loss: 0.07753 - acc: 0.9522 | val_loss: 0.63662 - val_acc: 0.8305 -- iter: 14848/55154\n",
      "--\n",
      "Training Step: 2703  | total loss: \u001b[1m\u001b[32m0.07739\u001b[0m\u001b[0m | time: 4475.540s\n",
      "| Adam | epoch: 007 | loss: 0.07739 - acc: 0.9523 | val_loss: 0.60321 - val_acc: 0.8348 -- iter: 14976/55154\n",
      "--\n",
      "Training Step: 2704  | total loss: \u001b[1m\u001b[32m0.07849\u001b[0m\u001b[0m | time: 4512.272s\n",
      "| Adam | epoch: 007 | loss: 0.07849 - acc: 0.9524 | val_loss: 0.54760 - val_acc: 0.8429 -- iter: 15104/55154\n",
      "--\n",
      "Training Step: 2705  | total loss: \u001b[1m\u001b[32m0.08086\u001b[0m\u001b[0m | time: 4549.117s\n",
      "| Adam | epoch: 007 | loss: 0.08086 - acc: 0.9517 | val_loss: 0.53915 - val_acc: 0.8430 -- iter: 15232/55154\n",
      "--\n",
      "Training Step: 2706  | total loss: \u001b[1m\u001b[32m0.07691\u001b[0m\u001b[0m | time: 4587.684s\n",
      "| Adam | epoch: 007 | loss: 0.07691 - acc: 0.9534 | val_loss: 0.55068 - val_acc: 0.8399 -- iter: 15360/55154\n",
      "--\n",
      "Training Step: 2707  | total loss: \u001b[1m\u001b[32m0.07461\u001b[0m\u001b[0m | time: 4627.072s\n",
      "| Adam | epoch: 007 | loss: 0.07461 - acc: 0.9526 | val_loss: 0.55283 - val_acc: 0.8400 -- iter: 15488/55154\n",
      "--\n",
      "Training Step: 2708  | total loss: \u001b[1m\u001b[32m0.07732\u001b[0m\u001b[0m | time: 4674.575s\n",
      "| Adam | epoch: 007 | loss: 0.07732 - acc: 0.9518 | val_loss: 0.52936 - val_acc: 0.8445 -- iter: 15616/55154\n",
      "--\n",
      "Training Step: 2709  | total loss: \u001b[1m\u001b[32m0.08158\u001b[0m\u001b[0m | time: 4719.464s\n",
      "| Adam | epoch: 007 | loss: 0.08158 - acc: 0.9512 | val_loss: 0.50502 - val_acc: 0.8522 -- iter: 15744/55154\n",
      "--\n",
      "Training Step: 2710  | total loss: \u001b[1m\u001b[32m0.08256\u001b[0m\u001b[0m | time: 4761.885s\n",
      "| Adam | epoch: 007 | loss: 0.08256 - acc: 0.9506 | val_loss: 0.52061 - val_acc: 0.8499 -- iter: 15872/55154\n",
      "--\n",
      "Training Step: 2711  | total loss: \u001b[1m\u001b[32m0.08139\u001b[0m\u001b[0m | time: 4799.374s\n",
      "| Adam | epoch: 007 | loss: 0.08139 - acc: 0.9509 | val_loss: 0.55501 - val_acc: 0.8441 -- iter: 16000/55154\n",
      "--\n",
      "Training Step: 2712  | total loss: \u001b[1m\u001b[32m0.07573\u001b[0m\u001b[0m | time: 4839.111s\n",
      "| Adam | epoch: 007 | loss: 0.07573 - acc: 0.9542 | val_loss: 0.57510 - val_acc: 0.8401 -- iter: 16128/55154\n",
      "--\n",
      "Training Step: 2713  | total loss: \u001b[1m\u001b[32m0.08129\u001b[0m\u001b[0m | time: 4877.557s\n",
      "| Adam | epoch: 007 | loss: 0.08129 - acc: 0.9541 | val_loss: 0.60060 - val_acc: 0.8350 -- iter: 16256/55154\n",
      "--\n",
      "Training Step: 2714  | total loss: \u001b[1m\u001b[32m0.07698\u001b[0m\u001b[0m | time: 4917.516s\n",
      "| Adam | epoch: 007 | loss: 0.07698 - acc: 0.9579 | val_loss: 0.63090 - val_acc: 0.8279 -- iter: 16384/55154\n",
      "--\n",
      "Training Step: 2715  | total loss: \u001b[1m\u001b[32m0.07347\u001b[0m\u001b[0m | time: 4955.507s\n",
      "| Adam | epoch: 007 | loss: 0.07347 - acc: 0.9566 | val_loss: 0.66516 - val_acc: 0.8228 -- iter: 16512/55154\n",
      "--\n",
      "Training Step: 2716  | total loss: \u001b[1m\u001b[32m0.07139\u001b[0m\u001b[0m | time: 4992.331s\n",
      "| Adam | epoch: 007 | loss: 0.07139 - acc: 0.9571 | val_loss: 0.64341 - val_acc: 0.8270 -- iter: 16640/55154\n",
      "--\n",
      "Training Step: 2717  | total loss: \u001b[1m\u001b[32m0.07752\u001b[0m\u001b[0m | time: 5029.904s\n",
      "| Adam | epoch: 007 | loss: 0.07752 - acc: 0.9559 | val_loss: 0.64561 - val_acc: 0.8272 -- iter: 16768/55154\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2718  | total loss: \u001b[1m\u001b[32m0.08100\u001b[0m\u001b[0m | time: 5067.666s\n",
      "| Adam | epoch: 007 | loss: 0.08100 - acc: 0.9548 | val_loss: 0.61906 - val_acc: 0.8297 -- iter: 16896/55154\n",
      "--\n",
      "Training Step: 2719  | total loss: \u001b[1m\u001b[32m0.08101\u001b[0m\u001b[0m | time: 5105.846s\n",
      "| Adam | epoch: 007 | loss: 0.08101 - acc: 0.9531 | val_loss: 0.56778 - val_acc: 0.8382 -- iter: 17024/55154\n",
      "--\n",
      "Training Step: 2720  | total loss: \u001b[1m\u001b[32m0.08118\u001b[0m\u001b[0m | time: 5146.909s\n",
      "| Adam | epoch: 007 | loss: 0.08118 - acc: 0.9539 | val_loss: 0.54581 - val_acc: 0.8415 -- iter: 17152/55154\n",
      "--\n",
      "Training Step: 2721  | total loss: \u001b[1m\u001b[32m0.08158\u001b[0m\u001b[0m | time: 5184.758s\n",
      "| Adam | epoch: 007 | loss: 0.08158 - acc: 0.9546 | val_loss: 0.54911 - val_acc: 0.8406 -- iter: 17280/55154\n",
      "--\n",
      "Training Step: 2722  | total loss: \u001b[1m\u001b[32m0.08148\u001b[0m\u001b[0m | time: 5225.351s\n",
      "| Adam | epoch: 007 | loss: 0.08148 - acc: 0.9552 | val_loss: 0.53582 - val_acc: 0.8427 -- iter: 17408/55154\n",
      "--\n",
      "Training Step: 2723  | total loss: \u001b[1m\u001b[32m0.07924\u001b[0m\u001b[0m | time: 5270.224s\n",
      "| Adam | epoch: 007 | loss: 0.07924 - acc: 0.9558 | val_loss: 0.51211 - val_acc: 0.8468 -- iter: 17536/55154\n",
      "--\n",
      "Training Step: 2724  | total loss: \u001b[1m\u001b[32m0.07874\u001b[0m\u001b[0m | time: 5307.192s\n",
      "| Adam | epoch: 007 | loss: 0.07874 - acc: 0.9540 | val_loss: 0.51607 - val_acc: 0.8473 -- iter: 17664/55154\n",
      "--\n",
      "Training Step: 2725  | total loss: \u001b[1m\u001b[32m0.07408\u001b[0m\u001b[0m | time: 5344.205s\n",
      "| Adam | epoch: 007 | loss: 0.07408 - acc: 0.9554 | val_loss: 0.52739 - val_acc: 0.8457 -- iter: 17792/55154\n",
      "--\n",
      "Training Step: 2726  | total loss: \u001b[1m\u001b[32m0.08696\u001b[0m\u001b[0m | time: 5381.563s\n",
      "| Adam | epoch: 007 | loss: 0.08696 - acc: 0.9537 | val_loss: 0.53508 - val_acc: 0.8442 -- iter: 17920/55154\n",
      "--\n",
      "Training Step: 2727  | total loss: \u001b[1m\u001b[32m0.09928\u001b[0m\u001b[0m | time: 5420.595s\n",
      "| Adam | epoch: 007 | loss: 0.09928 - acc: 0.9528 | val_loss: 0.55416 - val_acc: 0.8400 -- iter: 18048/55154\n",
      "--\n",
      "Training Step: 2728  | total loss: \u001b[1m\u001b[32m0.09500\u001b[0m\u001b[0m | time: 5459.538s\n",
      "| Adam | epoch: 007 | loss: 0.09500 - acc: 0.9552 | val_loss: 0.56508 - val_acc: 0.8379 -- iter: 18176/55154\n",
      "--\n",
      "Training Step: 2729  | total loss: \u001b[1m\u001b[32m0.09150\u001b[0m\u001b[0m | time: 5499.561s\n",
      "| Adam | epoch: 007 | loss: 0.09150 - acc: 0.9550 | val_loss: 0.55137 - val_acc: 0.8416 -- iter: 18304/55154\n",
      "--\n",
      "Training Step: 2730  | total loss: \u001b[1m\u001b[32m0.09279\u001b[0m\u001b[0m | time: 5539.613s\n",
      "| Adam | epoch: 007 | loss: 0.09279 - acc: 0.9525 | val_loss: 0.55779 - val_acc: 0.8410 -- iter: 18432/55154\n",
      "--\n",
      "Training Step: 2731  | total loss: \u001b[1m\u001b[32m0.08814\u001b[0m\u001b[0m | time: 5577.588s\n",
      "| Adam | epoch: 007 | loss: 0.08814 - acc: 0.9525 | val_loss: 0.58268 - val_acc: 0.8375 -- iter: 18560/55154\n",
      "--\n",
      "Training Step: 2732  | total loss: \u001b[1m\u001b[32m0.08507\u001b[0m\u001b[0m | time: 5614.720s\n",
      "| Adam | epoch: 007 | loss: 0.08507 - acc: 0.9502 | val_loss: 0.61830 - val_acc: 0.8320 -- iter: 18688/55154\n",
      "--\n",
      "Training Step: 2733  | total loss: \u001b[1m\u001b[32m0.07849\u001b[0m\u001b[0m | time: 5654.591s\n",
      "| Adam | epoch: 007 | loss: 0.07849 - acc: 0.9521 | val_loss: 0.60623 - val_acc: 0.8324 -- iter: 18816/55154\n",
      "--\n",
      "Training Step: 2734  | total loss: \u001b[1m\u001b[32m0.08205\u001b[0m\u001b[0m | time: 5696.747s\n",
      "| Adam | epoch: 007 | loss: 0.08205 - acc: 0.9514 | val_loss: 0.57085 - val_acc: 0.8368 -- iter: 18944/55154\n",
      "--\n",
      "Training Step: 2735  | total loss: \u001b[1m\u001b[32m0.08247\u001b[0m\u001b[0m | time: 5733.782s\n",
      "| Adam | epoch: 007 | loss: 0.08247 - acc: 0.9516 | val_loss: 0.56337 - val_acc: 0.8368 -- iter: 19072/55154\n",
      "--\n",
      "Training Step: 2736  | total loss: \u001b[1m\u001b[32m0.07713\u001b[0m\u001b[0m | time: 5770.555s\n",
      "| Adam | epoch: 007 | loss: 0.07713 - acc: 0.9533 | val_loss: 0.55562 - val_acc: 0.8360 -- iter: 19200/55154\n",
      "--\n",
      "Training Step: 2737  | total loss: \u001b[1m\u001b[32m0.07562\u001b[0m\u001b[0m | time: 5808.013s\n",
      "| Adam | epoch: 007 | loss: 0.07562 - acc: 0.9541 | val_loss: 0.56739 - val_acc: 0.8334 -- iter: 19328/55154\n",
      "--\n",
      "Training Step: 2738  | total loss: \u001b[1m\u001b[32m0.06986\u001b[0m\u001b[0m | time: 5845.022s\n",
      "| Adam | epoch: 007 | loss: 0.06986 - acc: 0.9548 | val_loss: 0.57090 - val_acc: 0.8317 -- iter: 19456/55154\n",
      "--\n",
      "Training Step: 2739  | total loss: \u001b[1m\u001b[32m0.07130\u001b[0m\u001b[0m | time: 5882.798s\n",
      "| Adam | epoch: 007 | loss: 0.07130 - acc: 0.9515 | val_loss: 0.56577 - val_acc: 0.8324 -- iter: 19584/55154\n",
      "--\n",
      "Training Step: 2740  | total loss: \u001b[1m\u001b[32m0.07006\u001b[0m\u001b[0m | time: 5920.731s\n",
      "| Adam | epoch: 007 | loss: 0.07006 - acc: 0.9524 | val_loss: 0.55322 - val_acc: 0.8363 -- iter: 19712/55154\n",
      "--\n",
      "Training Step: 2741  | total loss: \u001b[1m\u001b[32m0.06617\u001b[0m\u001b[0m | time: 5964.208s\n",
      "| Adam | epoch: 007 | loss: 0.06617 - acc: 0.9556 | val_loss: 0.52078 - val_acc: 0.8439 -- iter: 19840/55154\n",
      "--\n",
      "Training Step: 2742  | total loss: \u001b[1m\u001b[32m0.07476\u001b[0m\u001b[0m | time: 6001.336s\n",
      "| Adam | epoch: 007 | loss: 0.07476 - acc: 0.9530 | val_loss: 0.52332 - val_acc: 0.8453 -- iter: 19968/55154\n",
      "--\n",
      "Training Step: 2743  | total loss: \u001b[1m\u001b[32m0.07941\u001b[0m\u001b[0m | time: 6038.594s\n",
      "| Adam | epoch: 007 | loss: 0.07941 - acc: 0.9546 | val_loss: 0.55182 - val_acc: 0.8398 -- iter: 20096/55154\n",
      "--\n",
      "Training Step: 2744  | total loss: \u001b[1m\u001b[32m0.07618\u001b[0m\u001b[0m | time: 6075.613s\n",
      "| Adam | epoch: 007 | loss: 0.07618 - acc: 0.9537 | val_loss: 0.59263 - val_acc: 0.8333 -- iter: 20224/55154\n",
      "--\n",
      "Training Step: 2745  | total loss: \u001b[1m\u001b[32m0.07100\u001b[0m\u001b[0m | time: 6112.760s\n",
      "| Adam | epoch: 007 | loss: 0.07100 - acc: 0.9575 | val_loss: 0.61734 - val_acc: 0.8299 -- iter: 20352/55154\n",
      "--\n",
      "Training Step: 2746  | total loss: \u001b[1m\u001b[32m0.07480\u001b[0m\u001b[0m | time: 6149.985s\n",
      "| Adam | epoch: 007 | loss: 0.07480 - acc: 0.9540 | val_loss: 0.62550 - val_acc: 0.8272 -- iter: 20480/55154\n",
      "--\n",
      "Training Step: 2747  | total loss: \u001b[1m\u001b[32m0.08694\u001b[0m\u001b[0m | time: 6187.803s\n",
      "| Adam | epoch: 007 | loss: 0.08694 - acc: 0.9539 | val_loss: 0.60661 - val_acc: 0.8282 -- iter: 20608/55154\n",
      "--\n",
      "Training Step: 2748  | total loss: \u001b[1m\u001b[32m0.10055\u001b[0m\u001b[0m | time: 6230.502s\n",
      "| Adam | epoch: 007 | loss: 0.10055 - acc: 0.9491 | val_loss: 0.59512 - val_acc: 0.8293 -- iter: 20736/55154\n",
      "--\n",
      "Training Step: 2749  | total loss: \u001b[1m\u001b[32m0.09928\u001b[0m\u001b[0m | time: 6269.762s\n",
      "| Adam | epoch: 007 | loss: 0.09928 - acc: 0.9495 | val_loss: 0.59919 - val_acc: 0.8280 -- iter: 20864/55154\n",
      "--\n",
      "Training Step: 2750  | total loss: \u001b[1m\u001b[32m0.09612\u001b[0m\u001b[0m | time: 6308.454s\n",
      "| Adam | epoch: 007 | loss: 0.09612 - acc: 0.9499 | val_loss: 0.59463 - val_acc: 0.8284 -- iter: 20992/55154\n",
      "--\n",
      "Training Step: 2751  | total loss: \u001b[1m\u001b[32m0.09921\u001b[0m\u001b[0m | time: 6346.239s\n",
      "| Adam | epoch: 007 | loss: 0.09921 - acc: 0.9502 | val_loss: 0.60641 - val_acc: 0.8250 -- iter: 21120/55154\n",
      "--\n",
      "Training Step: 2752  | total loss: \u001b[1m\u001b[32m0.09875\u001b[0m\u001b[0m | time: 6383.422s\n",
      "| Adam | epoch: 007 | loss: 0.09875 - acc: 0.9505 | val_loss: 0.61927 - val_acc: 0.8206 -- iter: 21248/55154\n",
      "--\n",
      "Training Step: 2753  | total loss: \u001b[1m\u001b[32m0.09189\u001b[0m\u001b[0m | time: 6421.720s\n",
      "| Adam | epoch: 007 | loss: 0.09189 - acc: 0.9523 | val_loss: 0.57992 - val_acc: 0.8295 -- iter: 21376/55154\n",
      "--\n",
      "Training Step: 2754  | total loss: \u001b[1m\u001b[32m0.09705\u001b[0m\u001b[0m | time: 6461.841s\n",
      "| Adam | epoch: 007 | loss: 0.09705 - acc: 0.9485 | val_loss: 0.53517 - val_acc: 0.8390 -- iter: 21504/55154\n",
      "--\n",
      "Training Step: 2755  | total loss: \u001b[1m\u001b[32m0.10005\u001b[0m\u001b[0m | time: 6500.985s\n",
      "| Adam | epoch: 007 | loss: 0.10005 - acc: 0.9482 | val_loss: 0.50478 - val_acc: 0.8474 -- iter: 21632/55154\n",
      "--\n",
      "Training Step: 2756  | total loss: \u001b[1m\u001b[32m0.10319\u001b[0m\u001b[0m | time: 6538.738s\n",
      "| Adam | epoch: 007 | loss: 0.10319 - acc: 0.9440 | val_loss: 0.49827 - val_acc: 0.8495 -- iter: 21760/55154\n",
      "--\n",
      "Training Step: 2757  | total loss: \u001b[1m\u001b[32m0.09749\u001b[0m\u001b[0m | time: 6575.600s\n",
      "| Adam | epoch: 007 | loss: 0.09749 - acc: 0.9441 | val_loss: 0.50187 - val_acc: 0.8495 -- iter: 21888/55154\n",
      "--\n",
      "Training Step: 2758  | total loss: \u001b[1m\u001b[32m0.09537\u001b[0m\u001b[0m | time: 6612.605s\n",
      "| Adam | epoch: 007 | loss: 0.09537 - acc: 0.9466 | val_loss: 0.50466 - val_acc: 0.8483 -- iter: 22016/55154\n",
      "--\n",
      "Training Step: 2759  | total loss: \u001b[1m\u001b[32m0.09355\u001b[0m\u001b[0m | time: 6649.893s\n",
      "| Adam | epoch: 007 | loss: 0.09355 - acc: 0.9464 | val_loss: 0.51263 - val_acc: 0.8463 -- iter: 22144/55154\n",
      "--\n",
      "Training Step: 2760  | total loss: \u001b[1m\u001b[32m0.08933\u001b[0m\u001b[0m | time: 6686.972s\n",
      "| Adam | epoch: 007 | loss: 0.08933 - acc: 0.9479 | val_loss: 0.52163 - val_acc: 0.8444 -- iter: 22272/55154\n",
      "--\n",
      "Training Step: 2761  | total loss: \u001b[1m\u001b[32m0.08446\u001b[0m\u001b[0m | time: 6723.640s\n",
      "| Adam | epoch: 007 | loss: 0.08446 - acc: 0.9476 | val_loss: 0.50939 - val_acc: 0.8458 -- iter: 22400/55154\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2762  | total loss: \u001b[1m\u001b[32m0.08963\u001b[0m\u001b[0m | time: 6760.739s\n",
      "| Adam | epoch: 007 | loss: 0.08963 - acc: 0.9474 | val_loss: 0.52718 - val_acc: 0.8435 -- iter: 22528/55154\n",
      "--\n",
      "Training Step: 2763  | total loss: \u001b[1m\u001b[32m0.09017\u001b[0m\u001b[0m | time: 6798.291s\n",
      "| Adam | epoch: 007 | loss: 0.09017 - acc: 0.9472 | val_loss: 0.56442 - val_acc: 0.8373 -- iter: 22656/55154\n",
      "--\n",
      "Training Step: 2764  | total loss: \u001b[1m\u001b[32m0.08732\u001b[0m\u001b[0m | time: 6837.083s\n",
      "| Adam | epoch: 007 | loss: 0.08732 - acc: 0.9494 | val_loss: 0.53921 - val_acc: 0.8410 -- iter: 22784/55154\n",
      "--\n",
      "Training Step: 2765  | total loss: \u001b[1m\u001b[32m0.09564\u001b[0m\u001b[0m | time: 6875.181s\n",
      "| Adam | epoch: 007 | loss: 0.09564 - acc: 0.9474 | val_loss: 0.52328 - val_acc: 0.8426 -- iter: 22912/55154\n",
      "--\n",
      "Training Step: 2766  | total loss: \u001b[1m\u001b[32m0.09649\u001b[0m\u001b[0m | time: 6912.969s\n",
      "| Adam | epoch: 007 | loss: 0.09649 - acc: 0.9480 | val_loss: 0.50281 - val_acc: 0.8443 -- iter: 23040/55154\n",
      "--\n",
      "Training Step: 2767  | total loss: \u001b[1m\u001b[32m0.09543\u001b[0m\u001b[0m | time: 6950.344s\n",
      "| Adam | epoch: 007 | loss: 0.09543 - acc: 0.9485 | val_loss: 0.50556 - val_acc: 0.8422 -- iter: 23168/55154\n",
      "--\n",
      "Training Step: 2768  | total loss: \u001b[1m\u001b[32m0.09164\u001b[0m\u001b[0m | time: 6988.062s\n",
      "| Adam | epoch: 007 | loss: 0.09164 - acc: 0.9489 | val_loss: 0.51278 - val_acc: 0.8398 -- iter: 23296/55154\n",
      "--\n",
      "Training Step: 2769  | total loss: \u001b[1m\u001b[32m0.08480\u001b[0m\u001b[0m | time: 7025.658s\n",
      "| Adam | epoch: 007 | loss: 0.08480 - acc: 0.9517 | val_loss: 0.52471 - val_acc: 0.8377 -- iter: 23424/55154\n",
      "--\n",
      "Training Step: 2770  | total loss: \u001b[1m\u001b[32m0.08094\u001b[0m\u001b[0m | time: 7062.830s\n",
      "| Adam | epoch: 007 | loss: 0.08094 - acc: 0.9511 | val_loss: 0.53443 - val_acc: 0.8354 -- iter: 23552/55154\n",
      "--\n",
      "Training Step: 2771  | total loss: \u001b[1m\u001b[32m0.07917\u001b[0m\u001b[0m | time: 7100.481s\n",
      "| Adam | epoch: 007 | loss: 0.07917 - acc: 0.9513 | val_loss: 0.53171 - val_acc: 0.8365 -- iter: 23680/55154\n",
      "--\n",
      "Training Step: 2772  | total loss: \u001b[1m\u001b[32m0.08385\u001b[0m\u001b[0m | time: 7140.940s\n",
      "| Adam | epoch: 007 | loss: 0.08385 - acc: 0.9507 | val_loss: 0.50922 - val_acc: 0.8421 -- iter: 23808/55154\n",
      "--\n",
      "Training Step: 2773  | total loss: \u001b[1m\u001b[32m0.09049\u001b[0m\u001b[0m | time: 7178.885s\n",
      "| Adam | epoch: 007 | loss: 0.09049 - acc: 0.9478 | val_loss: 0.50105 - val_acc: 0.8452 -- iter: 23936/55154\n",
      "--\n",
      "Training Step: 2774  | total loss: \u001b[1m\u001b[32m0.09243\u001b[0m\u001b[0m | time: 7221.287s\n",
      "| Adam | epoch: 007 | loss: 0.09243 - acc: 0.9491 | val_loss: 0.51027 - val_acc: 0.8452 -- iter: 24064/55154\n",
      "--\n",
      "Training Step: 2775  | total loss: \u001b[1m\u001b[32m0.08922\u001b[0m\u001b[0m | time: 7263.734s\n",
      "| Adam | epoch: 007 | loss: 0.08922 - acc: 0.9479 | val_loss: 0.53290 - val_acc: 0.8418 -- iter: 24192/55154\n",
      "--\n",
      "Training Step: 2776  | total loss: \u001b[1m\u001b[32m0.08438\u001b[0m\u001b[0m | time: 7304.422s\n",
      "| Adam | epoch: 007 | loss: 0.08438 - acc: 0.9516 | val_loss: 0.55941 - val_acc: 0.8395 -- iter: 24320/55154\n",
      "--\n",
      "Training Step: 2777  | total loss: \u001b[1m\u001b[32m0.08300\u001b[0m\u001b[0m | time: 7344.639s\n",
      "| Adam | epoch: 007 | loss: 0.08300 - acc: 0.9510 | val_loss: 0.57537 - val_acc: 0.8374 -- iter: 24448/55154\n",
      "--\n",
      "Training Step: 2778  | total loss: \u001b[1m\u001b[32m0.08150\u001b[0m\u001b[0m | time: 7386.510s\n",
      "| Adam | epoch: 007 | loss: 0.08150 - acc: 0.9512 | val_loss: 0.56924 - val_acc: 0.8386 -- iter: 24576/55154\n",
      "--\n",
      "Training Step: 2779  | total loss: \u001b[1m\u001b[32m0.08154\u001b[0m\u001b[0m | time: 7428.646s\n",
      "| Adam | epoch: 007 | loss: 0.08154 - acc: 0.9506 | val_loss: 0.54459 - val_acc: 0.8442 -- iter: 24704/55154\n",
      "--\n",
      "Training Step: 2780  | total loss: \u001b[1m\u001b[32m0.08010\u001b[0m\u001b[0m | time: 7470.960s\n",
      "| Adam | epoch: 007 | loss: 0.08010 - acc: 0.9516 | val_loss: 0.51056 - val_acc: 0.8502 -- iter: 24832/55154\n",
      "--\n",
      "Training Step: 2781  | total loss: \u001b[1m\u001b[32m0.07663\u001b[0m\u001b[0m | time: 7509.157s\n",
      "| Adam | epoch: 007 | loss: 0.07663 - acc: 0.9533 | val_loss: 0.49036 - val_acc: 0.8533 -- iter: 24960/55154\n",
      "--\n",
      "Training Step: 2782  | total loss: \u001b[1m\u001b[32m0.07532\u001b[0m\u001b[0m | time: 7546.404s\n",
      "| Adam | epoch: 007 | loss: 0.07532 - acc: 0.9564 | val_loss: 0.48983 - val_acc: 0.8527 -- iter: 25088/55154\n",
      "--\n",
      "Training Step: 2783  | total loss: \u001b[1m\u001b[32m0.07379\u001b[0m\u001b[0m | time: 7583.523s\n",
      "| Adam | epoch: 007 | loss: 0.07379 - acc: 0.9577 | val_loss: 0.50470 - val_acc: 0.8491 -- iter: 25216/55154\n",
      "--\n",
      "Training Step: 2784  | total loss: \u001b[1m\u001b[32m0.07041\u001b[0m\u001b[0m | time: 7620.281s\n",
      "| Adam | epoch: 007 | loss: 0.07041 - acc: 0.9580 | val_loss: 0.53606 - val_acc: 0.8417 -- iter: 25344/55154\n",
      "--\n",
      "Training Step: 2785  | total loss: \u001b[1m\u001b[32m0.07276\u001b[0m\u001b[0m | time: 7657.295s\n",
      "| Adam | epoch: 007 | loss: 0.07276 - acc: 0.9575 | val_loss: 0.56208 - val_acc: 0.8379 -- iter: 25472/55154\n",
      "--\n",
      "Training Step: 2786  | total loss: \u001b[1m\u001b[32m0.07008\u001b[0m\u001b[0m | time: 7695.357s\n",
      "| Adam | epoch: 007 | loss: 0.07008 - acc: 0.9571 | val_loss: 0.55254 - val_acc: 0.8407 -- iter: 25600/55154\n",
      "--\n",
      "Training Step: 2787  | total loss: \u001b[1m\u001b[32m0.07994\u001b[0m\u001b[0m | time: 7736.683s\n",
      "| Adam | epoch: 007 | loss: 0.07994 - acc: 0.9551 | val_loss: 0.52495 - val_acc: 0.8471 -- iter: 25728/55154\n",
      "--\n",
      "Training Step: 2788  | total loss: \u001b[1m\u001b[32m0.07891\u001b[0m\u001b[0m | time: 7778.838s\n",
      "| Adam | epoch: 007 | loss: 0.07891 - acc: 0.9526 | val_loss: 0.50485 - val_acc: 0.8533 -- iter: 25856/55154\n",
      "--\n",
      "Training Step: 2789  | total loss: \u001b[1m\u001b[32m0.08493\u001b[0m\u001b[0m | time: 7821.510s\n",
      "| Adam | epoch: 007 | loss: 0.08493 - acc: 0.9534 | val_loss: 0.51513 - val_acc: 0.8527 -- iter: 25984/55154\n",
      "--\n",
      "Training Step: 2790  | total loss: \u001b[1m\u001b[32m0.08433\u001b[0m\u001b[0m | time: 7862.810s\n",
      "| Adam | epoch: 007 | loss: 0.08433 - acc: 0.9518 | val_loss: 0.52613 - val_acc: 0.8516 -- iter: 26112/55154\n",
      "--\n",
      "Training Step: 2791  | total loss: \u001b[1m\u001b[32m0.08334\u001b[0m\u001b[0m | time: 7903.309s\n",
      "| Adam | epoch: 007 | loss: 0.08334 - acc: 0.9535 | val_loss: 0.54415 - val_acc: 0.8495 -- iter: 26240/55154\n",
      "--\n",
      "Training Step: 2792  | total loss: \u001b[1m\u001b[32m0.08047\u001b[0m\u001b[0m | time: 7944.744s\n",
      "| Adam | epoch: 007 | loss: 0.08047 - acc: 0.9535 | val_loss: 0.57258 - val_acc: 0.8449 -- iter: 26368/55154\n",
      "--\n",
      "Training Step: 2793  | total loss: \u001b[1m\u001b[32m0.07759\u001b[0m\u001b[0m | time: 7983.217s\n",
      "| Adam | epoch: 007 | loss: 0.07759 - acc: 0.9527 | val_loss: 0.58906 - val_acc: 0.8422 -- iter: 26496/55154\n",
      "--\n",
      "Training Step: 2794  | total loss: \u001b[1m\u001b[32m0.08225\u001b[0m\u001b[0m | time: 8024.134s\n",
      "| Adam | epoch: 007 | loss: 0.08225 - acc: 0.9504 | val_loss: 0.59416 - val_acc: 0.8408 -- iter: 26624/55154\n",
      "--\n",
      "Training Step: 2795  | total loss: \u001b[1m\u001b[32m0.08235\u001b[0m\u001b[0m | time: 8063.147s\n",
      "| Adam | epoch: 007 | loss: 0.08235 - acc: 0.9475 | val_loss: 0.56113 - val_acc: 0.8451 -- iter: 26752/55154\n",
      "--\n",
      "Training Step: 2796  | total loss: \u001b[1m\u001b[32m0.08455\u001b[0m\u001b[0m | time: 8102.912s\n",
      "| Adam | epoch: 007 | loss: 0.08455 - acc: 0.9418 | val_loss: 0.53719 - val_acc: 0.8471 -- iter: 26880/55154\n",
      "--\n",
      "Training Step: 2797  | total loss: \u001b[1m\u001b[32m0.08482\u001b[0m\u001b[0m | time: 8140.577s\n",
      "| Adam | epoch: 007 | loss: 0.08482 - acc: 0.9430 | val_loss: 0.52642 - val_acc: 0.8465 -- iter: 27008/55154\n",
      "--\n",
      "Training Step: 2798  | total loss: \u001b[1m\u001b[32m0.08364\u001b[0m\u001b[0m | time: 8183.979s\n",
      "| Adam | epoch: 007 | loss: 0.08364 - acc: 0.9463 | val_loss: 0.54444 - val_acc: 0.8411 -- iter: 27136/55154\n",
      "--\n",
      "Training Step: 2799  | total loss: \u001b[1m\u001b[32m0.07943\u001b[0m\u001b[0m | time: 8224.658s\n",
      "| Adam | epoch: 007 | loss: 0.07943 - acc: 0.9462 | val_loss: 0.52402 - val_acc: 0.8447 -- iter: 27264/55154\n",
      "--\n",
      "Training Step: 2800  | total loss: \u001b[1m\u001b[32m0.08130\u001b[0m\u001b[0m | time: 8266.592s\n",
      "| Adam | epoch: 007 | loss: 0.08130 - acc: 0.9461 | val_loss: 0.52141 - val_acc: 0.8455 -- iter: 27392/55154\n",
      "--\n",
      "Training Step: 2801  | total loss: \u001b[1m\u001b[32m0.07942\u001b[0m\u001b[0m | time: 8309.303s\n",
      "| Adam | epoch: 007 | loss: 0.07942 - acc: 0.9492 | val_loss: 0.50505 - val_acc: 0.8475 -- iter: 27520/55154\n",
      "--\n",
      "Training Step: 2802  | total loss: \u001b[1m\u001b[32m0.08614\u001b[0m\u001b[0m | time: 8350.940s\n",
      "| Adam | epoch: 007 | loss: 0.08614 - acc: 0.9488 | val_loss: 0.54219 - val_acc: 0.8420 -- iter: 27648/55154\n",
      "--\n",
      "Training Step: 2803  | total loss: \u001b[1m\u001b[32m0.08464\u001b[0m\u001b[0m | time: 8392.537s\n",
      "| Adam | epoch: 007 | loss: 0.08464 - acc: 0.9492 | val_loss: 0.56045 - val_acc: 0.8394 -- iter: 27776/55154\n",
      "--\n",
      "Training Step: 2804  | total loss: \u001b[1m\u001b[32m0.09077\u001b[0m\u001b[0m | time: 8429.763s\n",
      "| Adam | epoch: 007 | loss: 0.09077 - acc: 0.9473 | val_loss: 0.56308 - val_acc: 0.8397 -- iter: 27904/55154\n",
      "--\n",
      "Training Step: 2805  | total loss: \u001b[1m\u001b[32m0.09063\u001b[0m\u001b[0m | time: 8466.746s\n",
      "| Adam | epoch: 007 | loss: 0.09063 - acc: 0.9502 | val_loss: 0.54727 - val_acc: 0.8420 -- iter: 28032/55154\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2806  | total loss: \u001b[1m\u001b[32m0.08499\u001b[0m\u001b[0m | time: 8504.105s\n",
      "| Adam | epoch: 007 | loss: 0.08499 - acc: 0.9536 | val_loss: 0.51695 - val_acc: 0.8493 -- iter: 28160/55154\n",
      "--\n",
      "Training Step: 2807  | total loss: \u001b[1m\u001b[32m0.08538\u001b[0m\u001b[0m | time: 8540.867s\n",
      "| Adam | epoch: 007 | loss: 0.08538 - acc: 0.9536 | val_loss: 0.49511 - val_acc: 0.8524 -- iter: 28288/55154\n",
      "--\n",
      "Training Step: 2808  | total loss: \u001b[1m\u001b[32m0.08825\u001b[0m\u001b[0m | time: 8579.087s\n",
      "| Adam | epoch: 007 | loss: 0.08825 - acc: 0.9535 | val_loss: 0.50633 - val_acc: 0.8483 -- iter: 28416/55154\n",
      "--\n",
      "Training Step: 2809  | total loss: \u001b[1m\u001b[32m0.08674\u001b[0m\u001b[0m | time: 8616.190s\n",
      "| Adam | epoch: 007 | loss: 0.08674 - acc: 0.9543 | val_loss: 0.51973 - val_acc: 0.8443 -- iter: 28544/55154\n",
      "--\n",
      "Training Step: 2810  | total loss: \u001b[1m\u001b[32m0.08518\u001b[0m\u001b[0m | time: 8654.570s\n",
      "| Adam | epoch: 007 | loss: 0.08518 - acc: 0.9541 | val_loss: 0.53633 - val_acc: 0.8404 -- iter: 28672/55154\n",
      "--\n",
      "Training Step: 2811  | total loss: \u001b[1m\u001b[32m0.08452\u001b[0m\u001b[0m | time: 8696.867s\n",
      "| Adam | epoch: 007 | loss: 0.08452 - acc: 0.9548 | val_loss: 0.55006 - val_acc: 0.8373 -- iter: 28800/55154\n",
      "--\n",
      "Training Step: 2812  | total loss: \u001b[1m\u001b[32m0.07990\u001b[0m\u001b[0m | time: 8739.009s\n",
      "| Adam | epoch: 007 | loss: 0.07990 - acc: 0.9547 | val_loss: 0.55685 - val_acc: 0.8357 -- iter: 28928/55154\n",
      "--\n",
      "Training Step: 2813  | total loss: \u001b[1m\u001b[32m0.07697\u001b[0m\u001b[0m | time: 8780.421s\n",
      "| Adam | epoch: 007 | loss: 0.07697 - acc: 0.9568 | val_loss: 0.55330 - val_acc: 0.8375 -- iter: 29056/55154\n",
      "--\n",
      "Training Step: 2814  | total loss: \u001b[1m\u001b[32m0.07293\u001b[0m\u001b[0m | time: 8822.686s\n",
      "| Adam | epoch: 007 | loss: 0.07293 - acc: 0.9588 | val_loss: 0.52281 - val_acc: 0.8438 -- iter: 29184/55154\n",
      "--\n",
      "Training Step: 2815  | total loss: \u001b[1m\u001b[32m0.07280\u001b[0m\u001b[0m | time: 8864.290s\n",
      "| Adam | epoch: 007 | loss: 0.07280 - acc: 0.9590 | val_loss: 0.47991 - val_acc: 0.8548 -- iter: 29312/55154\n",
      "--\n",
      "Training Step: 2816  | total loss: \u001b[1m\u001b[32m0.07667\u001b[0m\u001b[0m | time: 8901.329s\n",
      "| Adam | epoch: 007 | loss: 0.07667 - acc: 0.9569 | val_loss: 0.46194 - val_acc: 0.8591 -- iter: 29440/55154\n",
      "--\n",
      "Training Step: 2817  | total loss: \u001b[1m\u001b[32m0.07532\u001b[0m\u001b[0m | time: 8938.801s\n",
      "| Adam | epoch: 007 | loss: 0.07532 - acc: 0.9542 | val_loss: 0.46704 - val_acc: 0.8585 -- iter: 29568/55154\n",
      "--\n",
      "Training Step: 2818  | total loss: \u001b[1m\u001b[32m0.07026\u001b[0m\u001b[0m | time: 8978.693s\n",
      "| Adam | epoch: 007 | loss: 0.07026 - acc: 0.9525 | val_loss: 0.48171 - val_acc: 0.8550 -- iter: 29696/55154\n",
      "--\n",
      "Training Step: 2819  | total loss: \u001b[1m\u001b[32m0.06829\u001b[0m\u001b[0m | time: 9017.755s\n",
      "| Adam | epoch: 007 | loss: 0.06829 - acc: 0.9526 | val_loss: 0.50907 - val_acc: 0.8497 -- iter: 29824/55154\n",
      "--\n",
      "Training Step: 2820  | total loss: \u001b[1m\u001b[32m0.06502\u001b[0m\u001b[0m | time: 9060.144s\n",
      "| Adam | epoch: 007 | loss: 0.06502 - acc: 0.9542 | val_loss: 0.52671 - val_acc: 0.8467 -- iter: 29952/55154\n",
      "--\n",
      "Training Step: 2821  | total loss: \u001b[1m\u001b[32m0.07173\u001b[0m\u001b[0m | time: 9103.185s\n",
      "| Adam | epoch: 007 | loss: 0.07173 - acc: 0.9525 | val_loss: 0.52880 - val_acc: 0.8475 -- iter: 30080/55154\n",
      "--\n",
      "Training Step: 2822  | total loss: \u001b[1m\u001b[32m0.07291\u001b[0m\u001b[0m | time: 9144.033s\n",
      "| Adam | epoch: 007 | loss: 0.07291 - acc: 0.9526 | val_loss: 0.53263 - val_acc: 0.8479 -- iter: 30208/55154\n",
      "--\n",
      "Training Step: 2823  | total loss: \u001b[1m\u001b[32m0.07748\u001b[0m\u001b[0m | time: 9186.870s\n",
      "| Adam | epoch: 007 | loss: 0.07748 - acc: 0.9518 | val_loss: 0.53635 - val_acc: 0.8473 -- iter: 30336/55154\n",
      "--\n",
      "Training Step: 2824  | total loss: \u001b[1m\u001b[32m0.07355\u001b[0m\u001b[0m | time: 9229.835s\n",
      "| Adam | epoch: 007 | loss: 0.07355 - acc: 0.9535 | val_loss: 0.52324 - val_acc: 0.8505 -- iter: 30464/55154\n",
      "--\n",
      "Training Step: 2825  | total loss: \u001b[1m\u001b[32m0.07212\u001b[0m\u001b[0m | time: 9270.862s\n",
      "| Adam | epoch: 007 | loss: 0.07212 - acc: 0.9558 | val_loss: 0.52356 - val_acc: 0.8498 -- iter: 30592/55154\n",
      "--\n",
      "Training Step: 2826  | total loss: \u001b[1m\u001b[32m0.06918\u001b[0m\u001b[0m | time: 9316.257s\n",
      "| Adam | epoch: 007 | loss: 0.06918 - acc: 0.9571 | val_loss: 0.52244 - val_acc: 0.8520 -- iter: 30720/55154\n",
      "--\n",
      "Training Step: 2827  | total loss: \u001b[1m\u001b[32m0.06538\u001b[0m\u001b[0m | time: 9360.355s\n",
      "| Adam | epoch: 007 | loss: 0.06538 - acc: 0.9583 | val_loss: 0.52780 - val_acc: 0.8510 -- iter: 30848/55154\n",
      "--\n",
      "Training Step: 2828  | total loss: \u001b[1m\u001b[32m0.06228\u001b[0m\u001b[0m | time: 9399.238s\n",
      "| Adam | epoch: 007 | loss: 0.06228 - acc: 0.9609 | val_loss: 0.55090 - val_acc: 0.8466 -- iter: 30976/55154\n",
      "--\n",
      "Training Step: 2829  | total loss: \u001b[1m\u001b[32m0.06567\u001b[0m\u001b[0m | time: 9436.197s\n",
      "| Adam | epoch: 007 | loss: 0.06567 - acc: 0.9601 | val_loss: 0.55698 - val_acc: 0.8463 -- iter: 31104/55154\n",
      "--\n",
      "Training Step: 2830  | total loss: \u001b[1m\u001b[32m0.07192\u001b[0m\u001b[0m | time: 9473.042s\n",
      "| Adam | epoch: 007 | loss: 0.07192 - acc: 0.9579 | val_loss: 0.56435 - val_acc: 0.8449 -- iter: 31232/55154\n",
      "--\n",
      "Training Step: 2831  | total loss: \u001b[1m\u001b[32m0.06701\u001b[0m\u001b[0m | time: 9510.368s\n",
      "| Adam | epoch: 007 | loss: 0.06701 - acc: 0.9582 | val_loss: 0.54142 - val_acc: 0.8499 -- iter: 31360/55154\n",
      "--\n",
      "Training Step: 2832  | total loss: \u001b[1m\u001b[32m0.07950\u001b[0m\u001b[0m | time: 9547.562s\n",
      "| Adam | epoch: 007 | loss: 0.07950 - acc: 0.9577 | val_loss: 0.51331 - val_acc: 0.8542 -- iter: 31488/55154\n",
      "--\n",
      "Training Step: 2833  | total loss: \u001b[1m\u001b[32m0.08040\u001b[0m\u001b[0m | time: 9584.681s\n",
      "| Adam | epoch: 007 | loss: 0.08040 - acc: 0.9580 | val_loss: 0.51293 - val_acc: 0.8533 -- iter: 31616/55154\n",
      "--\n",
      "Training Step: 2834  | total loss: \u001b[1m\u001b[32m0.07547\u001b[0m\u001b[0m | time: 9621.864s\n",
      "| Adam | epoch: 007 | loss: 0.07547 - acc: 0.9575 | val_loss: 0.53035 - val_acc: 0.8499 -- iter: 31744/55154\n",
      "--\n",
      "Training Step: 2835  | total loss: \u001b[1m\u001b[32m0.07253\u001b[0m\u001b[0m | time: 9660.189s\n",
      "| Adam | epoch: 007 | loss: 0.07253 - acc: 0.9594 | val_loss: 0.54718 - val_acc: 0.8459 -- iter: 31872/55154\n",
      "--\n",
      "Training Step: 2836  | total loss: \u001b[1m\u001b[32m0.07250\u001b[0m\u001b[0m | time: 9700.643s\n",
      "| Adam | epoch: 007 | loss: 0.07250 - acc: 0.9603 | val_loss: 0.57142 - val_acc: 0.8398 -- iter: 32000/55154\n",
      "--\n",
      "Training Step: 2837  | total loss: \u001b[1m\u001b[32m0.06945\u001b[0m\u001b[0m | time: 9741.398s\n",
      "| Adam | epoch: 007 | loss: 0.06945 - acc: 0.9596 | val_loss: 0.60244 - val_acc: 0.8322 -- iter: 32128/55154\n",
      "--\n",
      "Training Step: 2838  | total loss: \u001b[1m\u001b[32m0.07597\u001b[0m\u001b[0m | time: 9780.242s\n",
      "| Adam | epoch: 007 | loss: 0.07597 - acc: 0.9590 | val_loss: 0.58757 - val_acc: 0.8344 -- iter: 32256/55154\n",
      "--\n",
      "Training Step: 2839  | total loss: \u001b[1m\u001b[32m0.08156\u001b[0m\u001b[0m | time: 9817.906s\n",
      "| Adam | epoch: 007 | loss: 0.08156 - acc: 0.9568 | val_loss: 0.55395 - val_acc: 0.8418 -- iter: 32384/55154\n",
      "--\n",
      "Training Step: 2840  | total loss: \u001b[1m\u001b[32m0.07803\u001b[0m\u001b[0m | time: 9860.047s\n",
      "| Adam | epoch: 007 | loss: 0.07803 - acc: 0.9604 | val_loss: 0.50334 - val_acc: 0.8530 -- iter: 32512/55154\n",
      "--\n",
      "Training Step: 2841  | total loss: \u001b[1m\u001b[32m0.08012\u001b[0m\u001b[0m | time: 9902.694s\n",
      "| Adam | epoch: 007 | loss: 0.08012 - acc: 0.9565 | val_loss: 0.47574 - val_acc: 0.8586 -- iter: 32640/55154\n",
      "--\n",
      "Training Step: 2842  | total loss: \u001b[1m\u001b[32m0.07722\u001b[0m\u001b[0m | time: 9942.679s\n",
      "| Adam | epoch: 007 | loss: 0.07722 - acc: 0.9562 | val_loss: 0.49099 - val_acc: 0.8561 -- iter: 32768/55154\n",
      "--\n",
      "Training Step: 2843  | total loss: \u001b[1m\u001b[32m0.08006\u001b[0m\u001b[0m | time: 9983.528s\n",
      "| Adam | epoch: 007 | loss: 0.08006 - acc: 0.9535 | val_loss: 0.53549 - val_acc: 0.8464 -- iter: 32896/55154\n",
      "--\n",
      "Training Step: 2844  | total loss: \u001b[1m\u001b[32m0.07782\u001b[0m\u001b[0m | time: 10025.168s\n",
      "| Adam | epoch: 007 | loss: 0.07782 - acc: 0.9527 | val_loss: 0.56480 - val_acc: 0.8420 -- iter: 33024/55154\n",
      "--\n",
      "Training Step: 2845  | total loss: \u001b[1m\u001b[32m0.07777\u001b[0m\u001b[0m | time: 10069.058s\n",
      "| Adam | epoch: 007 | loss: 0.07777 - acc: 0.9512 | val_loss: 0.60281 - val_acc: 0.8337 -- iter: 33152/55154\n",
      "--\n",
      "Training Step: 2846  | total loss: \u001b[1m\u001b[32m0.07437\u001b[0m\u001b[0m | time: 10111.671s\n",
      "| Adam | epoch: 007 | loss: 0.07437 - acc: 0.9522 | val_loss: 0.63776 - val_acc: 0.8268 -- iter: 33280/55154\n",
      "--\n",
      "Training Step: 2847  | total loss: \u001b[1m\u001b[32m0.07139\u001b[0m\u001b[0m | time: 10155.465s\n",
      "| Adam | epoch: 007 | loss: 0.07139 - acc: 0.9507 | val_loss: 0.65955 - val_acc: 0.8222 -- iter: 33408/55154\n",
      "--\n",
      "Training Step: 2848  | total loss: \u001b[1m\u001b[32m0.08683\u001b[0m\u001b[0m | time: 10194.690s\n",
      "| Adam | epoch: 007 | loss: 0.08683 - acc: 0.9486 | val_loss: 0.64208 - val_acc: 0.8261 -- iter: 33536/55154\n",
      "--\n",
      "Training Step: 2849  | total loss: \u001b[1m\u001b[32m0.08451\u001b[0m\u001b[0m | time: 10234.443s\n",
      "| Adam | epoch: 007 | loss: 0.08451 - acc: 0.9498 | val_loss: 0.59387 - val_acc: 0.8341 -- iter: 33664/55154\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2850  | total loss: \u001b[1m\u001b[32m0.08868\u001b[0m\u001b[0m | time: 10272.070s\n",
      "| Adam | epoch: 007 | loss: 0.08868 - acc: 0.9502 | val_loss: 0.57551 - val_acc: 0.8373 -- iter: 33792/55154\n",
      "--\n",
      "Training Step: 2851  | total loss: \u001b[1m\u001b[32m0.08688\u001b[0m\u001b[0m | time: 10309.852s\n",
      "| Adam | epoch: 007 | loss: 0.08688 - acc: 0.9497 | val_loss: 0.59376 - val_acc: 0.8338 -- iter: 33920/55154\n",
      "--\n",
      "Training Step: 2852  | total loss: \u001b[1m\u001b[32m0.08279\u001b[0m\u001b[0m | time: 10348.458s\n",
      "| Adam | epoch: 007 | loss: 0.08279 - acc: 0.9508 | val_loss: 0.63337 - val_acc: 0.8254 -- iter: 34048/55154\n",
      "--\n",
      "Training Step: 2853  | total loss: \u001b[1m\u001b[32m0.08166\u001b[0m\u001b[0m | time: 10387.849s\n",
      "| Adam | epoch: 007 | loss: 0.08166 - acc: 0.9526 | val_loss: 0.63764 - val_acc: 0.8241 -- iter: 34176/55154\n",
      "--\n",
      "Training Step: 2854  | total loss: \u001b[1m\u001b[32m0.09377\u001b[0m\u001b[0m | time: 10427.536s\n",
      "| Adam | epoch: 007 | loss: 0.09377 - acc: 0.9511 | val_loss: 0.61150 - val_acc: 0.8290 -- iter: 34304/55154\n",
      "--\n",
      "Training Step: 2855  | total loss: \u001b[1m\u001b[32m0.09615\u001b[0m\u001b[0m | time: 10468.060s\n",
      "| Adam | epoch: 007 | loss: 0.09615 - acc: 0.9505 | val_loss: 0.58986 - val_acc: 0.8338 -- iter: 34432/55154\n",
      "--\n",
      "Training Step: 2856  | total loss: \u001b[1m\u001b[32m0.09583\u001b[0m\u001b[0m | time: 10507.305s\n",
      "| Adam | epoch: 007 | loss: 0.09583 - acc: 0.9500 | val_loss: 0.59501 - val_acc: 0.8337 -- iter: 34560/55154\n",
      "--\n",
      "Training Step: 2857  | total loss: \u001b[1m\u001b[32m0.09377\u001b[0m\u001b[0m | time: 10544.415s\n",
      "| Adam | epoch: 007 | loss: 0.09377 - acc: 0.9526 | val_loss: 0.61314 - val_acc: 0.8306 -- iter: 34688/55154\n",
      "--\n",
      "Training Step: 2858  | total loss: \u001b[1m\u001b[32m0.08657\u001b[0m\u001b[0m | time: 10581.605s\n",
      "| Adam | epoch: 007 | loss: 0.08657 - acc: 0.9566 | val_loss: 0.62030 - val_acc: 0.8301 -- iter: 34816/55154\n",
      "--\n",
      "Training Step: 2859  | total loss: \u001b[1m\u001b[32m0.08505\u001b[0m\u001b[0m | time: 10618.584s\n",
      "| Adam | epoch: 007 | loss: 0.08505 - acc: 0.9578 | val_loss: 0.61517 - val_acc: 0.8316 -- iter: 34944/55154\n",
      "--\n",
      "Training Step: 2860  | total loss: \u001b[1m\u001b[32m0.07884\u001b[0m\u001b[0m | time: 10655.580s\n",
      "| Adam | epoch: 007 | loss: 0.07884 - acc: 0.9613 | val_loss: 0.59517 - val_acc: 0.8359 -- iter: 35072/55154\n",
      "--\n",
      "Training Step: 2861  | total loss: \u001b[1m\u001b[32m0.07951\u001b[0m\u001b[0m | time: 10692.479s\n",
      "| Adam | epoch: 007 | loss: 0.07951 - acc: 0.9565 | val_loss: 0.56782 - val_acc: 0.8394 -- iter: 35200/55154\n",
      "--\n",
      "Training Step: 2862  | total loss: \u001b[1m\u001b[32m0.08170\u001b[0m\u001b[0m | time: 10729.663s\n",
      "| Adam | epoch: 007 | loss: 0.08170 - acc: 0.9546 | val_loss: 0.55617 - val_acc: 0.8403 -- iter: 35328/55154\n",
      "--\n",
      "Training Step: 2863  | total loss: \u001b[1m\u001b[32m0.07637\u001b[0m\u001b[0m | time: 10766.988s\n",
      "| Adam | epoch: 007 | loss: 0.07637 - acc: 0.9576 | val_loss: 0.53697 - val_acc: 0.8433 -- iter: 35456/55154\n",
      "--\n",
      "Training Step: 2864  | total loss: \u001b[1m\u001b[32m0.07749\u001b[0m\u001b[0m | time: 10804.324s\n",
      "| Adam | epoch: 007 | loss: 0.07749 - acc: 0.9564 | val_loss: 0.50986 - val_acc: 0.8484 -- iter: 35584/55154\n",
      "--\n",
      "Training Step: 2865  | total loss: \u001b[1m\u001b[32m0.07543\u001b[0m\u001b[0m | time: 10841.772s\n",
      "| Adam | epoch: 007 | loss: 0.07543 - acc: 0.9568 | val_loss: 0.49140 - val_acc: 0.8527 -- iter: 35712/55154\n",
      "--\n",
      "Training Step: 2866  | total loss: \u001b[1m\u001b[32m0.07951\u001b[0m\u001b[0m | time: 10882.193s\n",
      "| Adam | epoch: 007 | loss: 0.07951 - acc: 0.9549 | val_loss: 0.49158 - val_acc: 0.8531 -- iter: 35840/55154\n",
      "--\n",
      "Training Step: 2867  | total loss: \u001b[1m\u001b[32m0.07658\u001b[0m\u001b[0m | time: 10925.474s\n",
      "| Adam | epoch: 007 | loss: 0.07658 - acc: 0.9555 | val_loss: 0.51341 - val_acc: 0.8485 -- iter: 35968/55154\n",
      "--\n",
      "Training Step: 2868  | total loss: \u001b[1m\u001b[32m0.07182\u001b[0m\u001b[0m | time: 10964.614s\n",
      "| Adam | epoch: 007 | loss: 0.07182 - acc: 0.9568 | val_loss: 0.55504 - val_acc: 0.8419 -- iter: 36096/55154\n",
      "--\n",
      "Training Step: 2869  | total loss: \u001b[1m\u001b[32m0.06940\u001b[0m\u001b[0m | time: 11004.643s\n",
      "| Adam | epoch: 007 | loss: 0.06940 - acc: 0.9565 | val_loss: 0.61629 - val_acc: 0.8298 -- iter: 36224/55154\n",
      "--\n",
      "Training Step: 2870  | total loss: \u001b[1m\u001b[32m0.07257\u001b[0m\u001b[0m | time: 11046.771s\n",
      "| Adam | epoch: 007 | loss: 0.07257 - acc: 0.9569 | val_loss: 0.62082 - val_acc: 0.8286 -- iter: 36352/55154\n",
      "--\n",
      "Training Step: 2871  | total loss: \u001b[1m\u001b[32m0.07647\u001b[0m\u001b[0m | time: 11091.895s\n",
      "| Adam | epoch: 007 | loss: 0.07647 - acc: 0.9503 | val_loss: 0.59215 - val_acc: 0.8342 -- iter: 36480/55154\n",
      "--\n",
      "Training Step: 2872  | total loss: \u001b[1m\u001b[32m0.07473\u001b[0m\u001b[0m | time: 11129.170s\n",
      "| Adam | epoch: 007 | loss: 0.07473 - acc: 0.9490 | val_loss: 0.57204 - val_acc: 0.8385 -- iter: 36608/55154\n",
      "--\n",
      "Training Step: 2873  | total loss: \u001b[1m\u001b[32m0.07401\u001b[0m\u001b[0m | time: 11167.748s\n",
      "| Adam | epoch: 007 | loss: 0.07401 - acc: 0.9518 | val_loss: 0.55505 - val_acc: 0.8423 -- iter: 36736/55154\n",
      "--\n",
      "Training Step: 2874  | total loss: \u001b[1m\u001b[32m0.07487\u001b[0m\u001b[0m | time: 11205.641s\n",
      "| Adam | epoch: 007 | loss: 0.07487 - acc: 0.9503 | val_loss: 0.54287 - val_acc: 0.8451 -- iter: 36864/55154\n",
      "--\n",
      "Training Step: 2875  | total loss: \u001b[1m\u001b[32m0.07357\u001b[0m\u001b[0m | time: 11243.851s\n",
      "| Adam | epoch: 007 | loss: 0.07357 - acc: 0.9490 | val_loss: 0.51053 - val_acc: 0.8505 -- iter: 36992/55154\n",
      "--\n",
      "Training Step: 2876  | total loss: \u001b[1m\u001b[32m0.08021\u001b[0m\u001b[0m | time: 11280.604s\n",
      "| Adam | epoch: 007 | loss: 0.08021 - acc: 0.9479 | val_loss: 0.49265 - val_acc: 0.8541 -- iter: 37120/55154\n",
      "--\n",
      "Training Step: 2877  | total loss: \u001b[1m\u001b[32m0.09277\u001b[0m\u001b[0m | time: 11319.265s\n",
      "| Adam | epoch: 007 | loss: 0.09277 - acc: 0.9445 | val_loss: 0.52698 - val_acc: 0.8444 -- iter: 37248/55154\n",
      "--\n",
      "Training Step: 2878  | total loss: \u001b[1m\u001b[32m0.09183\u001b[0m\u001b[0m | time: 11361.820s\n",
      "| Adam | epoch: 007 | loss: 0.09183 - acc: 0.9430 | val_loss: 0.59201 - val_acc: 0.8324 -- iter: 37376/55154\n",
      "--\n",
      "Training Step: 2879  | total loss: \u001b[1m\u001b[32m0.08873\u001b[0m\u001b[0m | time: 11403.924s\n",
      "| Adam | epoch: 007 | loss: 0.08873 - acc: 0.9417 | val_loss: 0.64011 - val_acc: 0.8219 -- iter: 37504/55154\n",
      "--\n",
      "Training Step: 2880  | total loss: \u001b[1m\u001b[32m0.08646\u001b[0m\u001b[0m | time: 11443.231s\n",
      "| Adam | epoch: 007 | loss: 0.08646 - acc: 0.9436 | val_loss: 0.64019 - val_acc: 0.8219 -- iter: 37632/55154\n",
      "--\n",
      "Training Step: 2881  | total loss: \u001b[1m\u001b[32m0.09243\u001b[0m\u001b[0m | time: 11482.902s\n",
      "| Adam | epoch: 007 | loss: 0.09243 - acc: 0.9438 | val_loss: 0.60632 - val_acc: 0.8277 -- iter: 37760/55154\n",
      "--\n",
      "Training Step: 2882  | total loss: \u001b[1m\u001b[32m0.10082\u001b[0m\u001b[0m | time: 11521.611s\n",
      "| Adam | epoch: 007 | loss: 0.10082 - acc: 0.9400 | val_loss: 0.59343 - val_acc: 0.8317 -- iter: 37888/55154\n",
      "--\n",
      "Training Step: 2883  | total loss: \u001b[1m\u001b[32m0.10178\u001b[0m\u001b[0m | time: 11559.507s\n",
      "| Adam | epoch: 007 | loss: 0.10178 - acc: 0.9398 | val_loss: 0.58959 - val_acc: 0.8320 -- iter: 38016/55154\n",
      "--\n",
      "Training Step: 2884  | total loss: \u001b[1m\u001b[32m0.10505\u001b[0m\u001b[0m | time: 11596.709s\n",
      "| Adam | epoch: 007 | loss: 0.10505 - acc: 0.9403 | val_loss: 0.58061 - val_acc: 0.8339 -- iter: 38144/55154\n",
      "--\n",
      "Training Step: 2885  | total loss: \u001b[1m\u001b[32m0.10398\u001b[0m\u001b[0m | time: 11634.184s\n",
      "| Adam | epoch: 007 | loss: 0.10398 - acc: 0.9416 | val_loss: 0.57966 - val_acc: 0.8339 -- iter: 38272/55154\n",
      "--\n",
      "Training Step: 2886  | total loss: \u001b[1m\u001b[32m0.09893\u001b[0m\u001b[0m | time: 11671.094s\n",
      "| Adam | epoch: 007 | loss: 0.09893 - acc: 0.9451 | val_loss: 0.56962 - val_acc: 0.8351 -- iter: 38400/55154\n",
      "--\n",
      "Training Step: 2887  | total loss: \u001b[1m\u001b[32m0.09767\u001b[0m\u001b[0m | time: 11708.296s\n",
      "| Adam | epoch: 007 | loss: 0.09767 - acc: 0.9467 | val_loss: 0.52486 - val_acc: 0.8446 -- iter: 38528/55154\n",
      "--\n",
      "Training Step: 2888  | total loss: \u001b[1m\u001b[32m0.10091\u001b[0m\u001b[0m | time: 11745.005s\n",
      "| Adam | epoch: 007 | loss: 0.10091 - acc: 0.9458 | val_loss: 0.51568 - val_acc: 0.8476 -- iter: 38656/55154\n",
      "--\n",
      "Training Step: 2889  | total loss: \u001b[1m\u001b[32m0.09569\u001b[0m\u001b[0m | time: 11781.807s\n",
      "| Adam | epoch: 007 | loss: 0.09569 - acc: 0.9496 | val_loss: 0.53079 - val_acc: 0.8445 -- iter: 38784/55154\n",
      "--\n",
      "Training Step: 2890  | total loss: \u001b[1m\u001b[32m0.09465\u001b[0m\u001b[0m | time: 11818.962s\n",
      "| Adam | epoch: 007 | loss: 0.09465 - acc: 0.9515 | val_loss: 0.55177 - val_acc: 0.8413 -- iter: 38912/55154\n",
      "--\n",
      "Training Step: 2891  | total loss: \u001b[1m\u001b[32m0.08898\u001b[0m\u001b[0m | time: 11856.094s\n",
      "| Adam | epoch: 007 | loss: 0.08898 - acc: 0.9517 | val_loss: 0.58664 - val_acc: 0.8359 -- iter: 39040/55154\n",
      "--\n",
      "Training Step: 2892  | total loss: \u001b[1m\u001b[32m0.08147\u001b[0m\u001b[0m | time: 11893.716s\n",
      "| Adam | epoch: 007 | loss: 0.08147 - acc: 0.9558 | val_loss: 0.61485 - val_acc: 0.8316 -- iter: 39168/55154\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2893  | total loss: \u001b[1m\u001b[32m0.08144\u001b[0m\u001b[0m | time: 11931.408s\n",
      "| Adam | epoch: 007 | loss: 0.08144 - acc: 0.9563 | val_loss: 0.60072 - val_acc: 0.8346 -- iter: 39296/55154\n",
      "--\n",
      "Training Step: 2894  | total loss: \u001b[1m\u001b[32m0.08161\u001b[0m\u001b[0m | time: 11968.474s\n",
      "| Adam | epoch: 007 | loss: 0.08161 - acc: 0.9552 | val_loss: 0.57047 - val_acc: 0.8399 -- iter: 39424/55154\n",
      "--\n",
      "Training Step: 2895  | total loss: \u001b[1m\u001b[32m0.08786\u001b[0m\u001b[0m | time: 12005.650s\n",
      "| Adam | epoch: 007 | loss: 0.08786 - acc: 0.9550 | val_loss: 0.54154 - val_acc: 0.8438 -- iter: 39552/55154\n",
      "--\n",
      "Training Step: 2896  | total loss: \u001b[1m\u001b[32m0.09232\u001b[0m\u001b[0m | time: 12042.429s\n",
      "| Adam | epoch: 007 | loss: 0.09232 - acc: 0.9556 | val_loss: 0.54135 - val_acc: 0.8437 -- iter: 39680/55154\n",
      "--\n",
      "Training Step: 2897  | total loss: \u001b[1m\u001b[32m0.09272\u001b[0m\u001b[0m | time: 12080.528s\n",
      "| Adam | epoch: 007 | loss: 0.09272 - acc: 0.9584 | val_loss: 0.55852 - val_acc: 0.8404 -- iter: 39808/55154\n",
      "--\n",
      "Training Step: 2898  | total loss: \u001b[1m\u001b[32m0.09003\u001b[0m\u001b[0m | time: 12120.074s\n",
      "| Adam | epoch: 007 | loss: 0.09003 - acc: 0.9587 | val_loss: 0.55608 - val_acc: 0.8401 -- iter: 39936/55154\n",
      "--\n",
      "Training Step: 2899  | total loss: \u001b[1m\u001b[32m0.09881\u001b[0m\u001b[0m | time: 12163.445s\n",
      "| Adam | epoch: 007 | loss: 0.09881 - acc: 0.9542 | val_loss: 0.54554 - val_acc: 0.8428 -- iter: 40064/55154\n",
      "--\n",
      "Training Step: 2900  | total loss: \u001b[1m\u001b[32m0.10485\u001b[0m\u001b[0m | time: 12201.541s\n",
      "| Adam | epoch: 007 | loss: 0.10485 - acc: 0.9518 | val_loss: 0.52105 - val_acc: 0.8481 -- iter: 40192/55154\n",
      "--\n",
      "Training Step: 2901  | total loss: \u001b[1m\u001b[32m0.10653\u001b[0m\u001b[0m | time: 12238.732s\n",
      "| Adam | epoch: 007 | loss: 0.10653 - acc: 0.9488 | val_loss: 0.49960 - val_acc: 0.8511 -- iter: 40320/55154\n",
      "--\n",
      "Training Step: 2902  | total loss: \u001b[1m\u001b[32m0.10137\u001b[0m\u001b[0m | time: 12275.617s\n",
      "| Adam | epoch: 007 | loss: 0.10137 - acc: 0.9523 | val_loss: 0.49151 - val_acc: 0.8528 -- iter: 40448/55154\n",
      "--\n",
      "Training Step: 2903  | total loss: \u001b[1m\u001b[32m0.09364\u001b[0m\u001b[0m | time: 12312.402s\n",
      "| Adam | epoch: 007 | loss: 0.09364 - acc: 0.9532 | val_loss: 0.49730 - val_acc: 0.8509 -- iter: 40576/55154\n",
      "--\n",
      "Training Step: 2904  | total loss: \u001b[1m\u001b[32m0.08748\u001b[0m\u001b[0m | time: 12349.349s\n",
      "| Adam | epoch: 007 | loss: 0.08748 - acc: 0.9540 | val_loss: 0.51401 - val_acc: 0.8479 -- iter: 40704/55154\n",
      "--\n",
      "Training Step: 2905  | total loss: \u001b[1m\u001b[32m0.08386\u001b[0m\u001b[0m | time: 12386.311s\n",
      "| Adam | epoch: 007 | loss: 0.08386 - acc: 0.9555 | val_loss: 0.53800 - val_acc: 0.8431 -- iter: 40832/55154\n",
      "--\n",
      "Training Step: 2906  | total loss: \u001b[1m\u001b[32m0.08182\u001b[0m\u001b[0m | time: 12424.301s\n",
      "| Adam | epoch: 007 | loss: 0.08182 - acc: 0.9552 | val_loss: 0.54909 - val_acc: 0.8413 -- iter: 40960/55154\n",
      "--\n",
      "Training Step: 2907  | total loss: \u001b[1m\u001b[32m0.08340\u001b[0m\u001b[0m | time: 12463.967s\n",
      "| Adam | epoch: 007 | loss: 0.08340 - acc: 0.9581 | val_loss: 0.53364 - val_acc: 0.8446 -- iter: 41088/55154\n",
      "--\n",
      "Training Step: 2908  | total loss: \u001b[1m\u001b[32m0.07985\u001b[0m\u001b[0m | time: 12502.293s\n",
      "| Adam | epoch: 007 | loss: 0.07985 - acc: 0.9584 | val_loss: 0.51954 - val_acc: 0.8487 -- iter: 41216/55154\n",
      "--\n",
      "Training Step: 2909  | total loss: \u001b[1m\u001b[32m0.07666\u001b[0m\u001b[0m | time: 12539.501s\n",
      "| Adam | epoch: 007 | loss: 0.07666 - acc: 0.9555 | val_loss: 0.49965 - val_acc: 0.8534 -- iter: 41344/55154\n",
      "--\n",
      "Training Step: 2910  | total loss: \u001b[1m\u001b[32m0.07922\u001b[0m\u001b[0m | time: 12576.418s\n",
      "| Adam | epoch: 007 | loss: 0.07922 - acc: 0.9576 | val_loss: 0.49959 - val_acc: 0.8527 -- iter: 41472/55154\n",
      "--\n",
      "Training Step: 2911  | total loss: \u001b[1m\u001b[32m0.07381\u001b[0m\u001b[0m | time: 12617.102s\n",
      "| Adam | epoch: 007 | loss: 0.07381 - acc: 0.9564 | val_loss: 0.50800 - val_acc: 0.8508 -- iter: 41600/55154\n",
      "--\n",
      "Training Step: 2912  | total loss: \u001b[1m\u001b[32m0.07943\u001b[0m\u001b[0m | time: 12658.791s\n",
      "| Adam | epoch: 007 | loss: 0.07943 - acc: 0.9545 | val_loss: 0.53133 - val_acc: 0.8440 -- iter: 41728/55154\n",
      "--\n",
      "Training Step: 2913  | total loss: \u001b[1m\u001b[32m0.07707\u001b[0m\u001b[0m | time: 12705.145s\n",
      "| Adam | epoch: 007 | loss: 0.07707 - acc: 0.9536 | val_loss: 0.54680 - val_acc: 0.8397 -- iter: 41856/55154\n",
      "--\n",
      "Training Step: 2914  | total loss: \u001b[1m\u001b[32m0.07760\u001b[0m\u001b[0m | time: 12747.059s\n",
      "| Adam | epoch: 007 | loss: 0.07760 - acc: 0.9543 | val_loss: 0.54720 - val_acc: 0.8404 -- iter: 41984/55154\n",
      "--\n",
      "Training Step: 2915  | total loss: \u001b[1m\u001b[32m0.07736\u001b[0m\u001b[0m | time: 12789.065s\n",
      "| Adam | epoch: 007 | loss: 0.07736 - acc: 0.9566 | val_loss: 0.55059 - val_acc: 0.8393 -- iter: 42112/55154\n",
      "--\n",
      "Training Step: 2916  | total loss: \u001b[1m\u001b[32m0.07357\u001b[0m\u001b[0m | time: 12830.078s\n",
      "| Adam | epoch: 007 | loss: 0.07357 - acc: 0.9578 | val_loss: 0.51307 - val_acc: 0.8474 -- iter: 42240/55154\n",
      "--\n",
      "Training Step: 2917  | total loss: \u001b[1m\u001b[32m0.07862\u001b[0m\u001b[0m | time: 12873.151s\n",
      "| Adam | epoch: 007 | loss: 0.07862 - acc: 0.9550 | val_loss: 0.49244 - val_acc: 0.8517 -- iter: 42368/55154\n",
      "--\n",
      "Training Step: 2918  | total loss: \u001b[1m\u001b[32m0.08751\u001b[0m\u001b[0m | time: 12920.612s\n",
      "| Adam | epoch: 007 | loss: 0.08751 - acc: 0.9532 | val_loss: 0.48270 - val_acc: 0.8523 -- iter: 42496/55154\n",
      "--\n",
      "Training Step: 2919  | total loss: \u001b[1m\u001b[32m0.08611\u001b[0m\u001b[0m | time: 12968.664s\n",
      "| Adam | epoch: 007 | loss: 0.08611 - acc: 0.9524 | val_loss: 0.46713 - val_acc: 0.8555 -- iter: 42624/55154\n",
      "--\n",
      "Training Step: 2920  | total loss: \u001b[1m\u001b[32m0.08408\u001b[0m\u001b[0m | time: 13009.470s\n",
      "| Adam | epoch: 007 | loss: 0.08408 - acc: 0.9533 | val_loss: 0.46600 - val_acc: 0.8564 -- iter: 42752/55154\n",
      "--\n",
      "Training Step: 2921  | total loss: \u001b[1m\u001b[32m0.07848\u001b[0m\u001b[0m | time: 13048.689s\n",
      "| Adam | epoch: 007 | loss: 0.07848 - acc: 0.9540 | val_loss: 0.46430 - val_acc: 0.8568 -- iter: 42880/55154\n",
      "--\n",
      "Training Step: 2922  | total loss: \u001b[1m\u001b[32m0.07855\u001b[0m\u001b[0m | time: 13090.658s\n",
      "| Adam | epoch: 007 | loss: 0.07855 - acc: 0.9524 | val_loss: 0.46306 - val_acc: 0.8577 -- iter: 43008/55154\n",
      "--\n",
      "Training Step: 2923  | total loss: \u001b[1m\u001b[32m0.07825\u001b[0m\u001b[0m | time: 13130.443s\n",
      "| Adam | epoch: 007 | loss: 0.07825 - acc: 0.9540 | val_loss: 0.47130 - val_acc: 0.8564 -- iter: 43136/55154\n",
      "--\n",
      "Training Step: 2924  | total loss: \u001b[1m\u001b[32m0.07423\u001b[0m\u001b[0m | time: 13169.727s\n",
      "| Adam | epoch: 007 | loss: 0.07423 - acc: 0.9555 | val_loss: 0.48529 - val_acc: 0.8531 -- iter: 43264/55154\n",
      "--\n",
      "Training Step: 2925  | total loss: \u001b[1m\u001b[32m0.07149\u001b[0m\u001b[0m | time: 13213.447s\n",
      "| Adam | epoch: 007 | loss: 0.07149 - acc: 0.9592 | val_loss: 0.48343 - val_acc: 0.8542 -- iter: 43392/55154\n",
      "--\n",
      "Training Step: 2926  | total loss: \u001b[1m\u001b[32m0.07510\u001b[0m\u001b[0m | time: 13253.539s\n",
      "| Adam | epoch: 007 | loss: 0.07510 - acc: 0.9586 | val_loss: 0.46706 - val_acc: 0.8581 -- iter: 43520/55154\n",
      "--\n",
      "Training Step: 2927  | total loss: \u001b[1m\u001b[32m0.07105\u001b[0m\u001b[0m | time: 13293.316s\n",
      "| Adam | epoch: 007 | loss: 0.07105 - acc: 0.9596 | val_loss: 0.44853 - val_acc: 0.8626 -- iter: 43648/55154\n",
      "--\n",
      "Training Step: 2928  | total loss: \u001b[1m\u001b[32m0.07181\u001b[0m\u001b[0m | time: 13334.277s\n",
      "| Adam | epoch: 007 | loss: 0.07181 - acc: 0.9589 | val_loss: 0.43756 - val_acc: 0.8659 -- iter: 43776/55154\n",
      "--\n",
      "Training Step: 2929  | total loss: \u001b[1m\u001b[32m0.07401\u001b[0m\u001b[0m | time: 13378.936s\n",
      "| Adam | epoch: 007 | loss: 0.07401 - acc: 0.9599 | val_loss: 0.43549 - val_acc: 0.8660 -- iter: 43904/55154\n",
      "--\n",
      "Training Step: 2930  | total loss: \u001b[1m\u001b[32m0.06995\u001b[0m\u001b[0m | time: 13418.364s\n",
      "| Adam | epoch: 007 | loss: 0.06995 - acc: 0.9585 | val_loss: 0.44581 - val_acc: 0.8636 -- iter: 44032/55154\n",
      "--\n",
      "Training Step: 2931  | total loss: \u001b[1m\u001b[32m0.06834\u001b[0m\u001b[0m | time: 13457.780s\n",
      "| Adam | epoch: 007 | loss: 0.06834 - acc: 0.9587 | val_loss: 0.45085 - val_acc: 0.8620 -- iter: 44160/55154\n",
      "--\n",
      "Training Step: 2932  | total loss: \u001b[1m\u001b[32m0.06908\u001b[0m\u001b[0m | time: 13496.970s\n",
      "| Adam | epoch: 007 | loss: 0.06908 - acc: 0.9605 | val_loss: 0.44450 - val_acc: 0.8628 -- iter: 44288/55154\n",
      "--\n",
      "Training Step: 2933  | total loss: \u001b[1m\u001b[32m0.07594\u001b[0m\u001b[0m | time: 13539.321s\n",
      "| Adam | epoch: 007 | loss: 0.07594 - acc: 0.9566 | val_loss: 0.44153 - val_acc: 0.8640 -- iter: 44416/55154\n",
      "--\n",
      "Training Step: 2934  | total loss: \u001b[1m\u001b[32m0.07710\u001b[0m\u001b[0m | time: 13580.012s\n",
      "| Adam | epoch: 007 | loss: 0.07710 - acc: 0.9571 | val_loss: 0.46227 - val_acc: 0.8597 -- iter: 44544/55154\n",
      "--\n",
      "Training Step: 2935  | total loss: \u001b[1m\u001b[32m0.07379\u001b[0m\u001b[0m | time: 13619.711s\n",
      "| Adam | epoch: 007 | loss: 0.07379 - acc: 0.9590 | val_loss: 0.48771 - val_acc: 0.8537 -- iter: 44672/55154\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2936  | total loss: \u001b[1m\u001b[32m0.07015\u001b[0m\u001b[0m | time: 13666.674s\n",
      "| Adam | epoch: 007 | loss: 0.07015 - acc: 0.9615 | val_loss: 0.51969 - val_acc: 0.8461 -- iter: 44800/55154\n",
      "--\n",
      "Training Step: 2937  | total loss: \u001b[1m\u001b[32m0.06593\u001b[0m\u001b[0m | time: 13706.578s\n",
      "| Adam | epoch: 007 | loss: 0.06593 - acc: 0.9623 | val_loss: 0.55532 - val_acc: 0.8382 -- iter: 44928/55154\n",
      "--\n",
      "Training Step: 2938  | total loss: \u001b[1m\u001b[32m0.06032\u001b[0m\u001b[0m | time: 13748.989s\n",
      "| Adam | epoch: 007 | loss: 0.06032 - acc: 0.9614 | val_loss: 0.58943 - val_acc: 0.8316 -- iter: 45056/55154\n",
      "--\n",
      "Training Step: 2939  | total loss: \u001b[1m\u001b[32m0.06031\u001b[0m\u001b[0m | time: 13786.936s\n",
      "| Adam | epoch: 007 | loss: 0.06031 - acc: 0.9605 | val_loss: 0.59873 - val_acc: 0.8306 -- iter: 45184/55154\n",
      "--\n",
      "Training Step: 2940  | total loss: \u001b[1m\u001b[32m0.06504\u001b[0m\u001b[0m | time: 13825.125s\n",
      "| Adam | epoch: 007 | loss: 0.06504 - acc: 0.9606 | val_loss: 0.55703 - val_acc: 0.8379 -- iter: 45312/55154\n",
      "--\n",
      "Training Step: 2941  | total loss: \u001b[1m\u001b[32m0.07530\u001b[0m\u001b[0m | time: 13863.125s\n",
      "| Adam | epoch: 007 | loss: 0.07530 - acc: 0.9590 | val_loss: 0.50890 - val_acc: 0.8473 -- iter: 45440/55154\n",
      "--\n",
      "Training Step: 2942  | total loss: \u001b[1m\u001b[32m0.07485\u001b[0m\u001b[0m | time: 13902.488s\n",
      "| Adam | epoch: 007 | loss: 0.07485 - acc: 0.9608 | val_loss: 0.49284 - val_acc: 0.8511 -- iter: 45568/55154\n",
      "--\n",
      "Training Step: 2943  | total loss: \u001b[1m\u001b[32m0.07417\u001b[0m\u001b[0m | time: 13941.453s\n",
      "| Adam | epoch: 007 | loss: 0.07417 - acc: 0.9616 | val_loss: 0.49729 - val_acc: 0.8500 -- iter: 45696/55154\n",
      "--\n",
      "Training Step: 2944  | total loss: \u001b[1m\u001b[32m0.06986\u001b[0m\u001b[0m | time: 13981.779s\n",
      "| Adam | epoch: 007 | loss: 0.06986 - acc: 0.9639 | val_loss: 0.50726 - val_acc: 0.8478 -- iter: 45824/55154\n",
      "--\n",
      "Training Step: 2945  | total loss: \u001b[1m\u001b[32m0.06591\u001b[0m\u001b[0m | time: 14021.289s\n",
      "| Adam | epoch: 007 | loss: 0.06591 - acc: 0.9644 | val_loss: 0.48224 - val_acc: 0.8547 -- iter: 45952/55154\n",
      "--\n",
      "Training Step: 2946  | total loss: \u001b[1m\u001b[32m0.07773\u001b[0m\u001b[0m | time: 14059.538s\n",
      "| Adam | epoch: 007 | loss: 0.07773 - acc: 0.9570 | val_loss: 0.48457 - val_acc: 0.8553 -- iter: 46080/55154\n",
      "--\n",
      "Training Step: 2947  | total loss: \u001b[1m\u001b[32m0.08108\u001b[0m\u001b[0m | time: 14099.629s\n",
      "| Adam | epoch: 007 | loss: 0.08108 - acc: 0.9543 | val_loss: 0.53660 - val_acc: 0.8444 -- iter: 46208/55154\n",
      "--\n",
      "Training Step: 2948  | total loss: \u001b[1m\u001b[32m0.09717\u001b[0m\u001b[0m | time: 14138.090s\n",
      "| Adam | epoch: 007 | loss: 0.09717 - acc: 0.9502 | val_loss: 0.59870 - val_acc: 0.8324 -- iter: 46336/55154\n",
      "--\n",
      "Training Step: 2949  | total loss: \u001b[1m\u001b[32m0.09892\u001b[0m\u001b[0m | time: 14176.859s\n",
      "| Adam | epoch: 007 | loss: 0.09892 - acc: 0.9497 | val_loss: 0.57757 - val_acc: 0.8363 -- iter: 46464/55154\n",
      "--\n",
      "Training Step: 2950  | total loss: \u001b[1m\u001b[32m0.09740\u001b[0m\u001b[0m | time: 14218.405s\n",
      "| Adam | epoch: 007 | loss: 0.09740 - acc: 0.9509 | val_loss: 0.54966 - val_acc: 0.8427 -- iter: 46592/55154\n",
      "--\n",
      "Training Step: 2951  | total loss: \u001b[1m\u001b[32m0.09100\u001b[0m\u001b[0m | time: 14258.263s\n",
      "| Adam | epoch: 007 | loss: 0.09100 - acc: 0.9534 | val_loss: 0.53314 - val_acc: 0.8436 -- iter: 46720/55154\n",
      "--\n",
      "Training Step: 2952  | total loss: \u001b[1m\u001b[32m0.08477\u001b[0m\u001b[0m | time: 14298.825s\n",
      "| Adam | epoch: 007 | loss: 0.08477 - acc: 0.9565 | val_loss: 0.52506 - val_acc: 0.8436 -- iter: 46848/55154\n",
      "--\n",
      "Training Step: 2953  | total loss: \u001b[1m\u001b[32m0.08773\u001b[0m\u001b[0m | time: 14340.210s\n",
      "| Adam | epoch: 007 | loss: 0.08773 - acc: 0.9538 | val_loss: 0.51239 - val_acc: 0.8475 -- iter: 46976/55154\n",
      "--\n",
      "Training Step: 2954  | total loss: \u001b[1m\u001b[32m0.09012\u001b[0m\u001b[0m | time: 14379.861s\n",
      "| Adam | epoch: 007 | loss: 0.09012 - acc: 0.9514 | val_loss: 0.50973 - val_acc: 0.8498 -- iter: 47104/55154\n",
      "--\n",
      "Training Step: 2955  | total loss: \u001b[1m\u001b[32m0.08553\u001b[0m\u001b[0m | time: 14418.315s\n",
      "| Adam | epoch: 007 | loss: 0.08553 - acc: 0.9524 | val_loss: 0.52388 - val_acc: 0.8476 -- iter: 47232/55154\n",
      "--\n",
      "Training Step: 2956  | total loss: \u001b[1m\u001b[32m0.08082\u001b[0m\u001b[0m | time: 14456.456s\n",
      "| Adam | epoch: 007 | loss: 0.08082 - acc: 0.9485 | val_loss: 0.54393 - val_acc: 0.8449 -- iter: 47360/55154\n",
      "--\n",
      "Training Step: 2957  | total loss: \u001b[1m\u001b[32m0.07918\u001b[0m\u001b[0m | time: 14495.118s\n",
      "| Adam | epoch: 007 | loss: 0.07918 - acc: 0.9490 | val_loss: 0.56636 - val_acc: 0.8413 -- iter: 47488/55154\n",
      "--\n",
      "Training Step: 2958  | total loss: \u001b[1m\u001b[32m0.07455\u001b[0m\u001b[0m | time: 14533.888s\n",
      "| Adam | epoch: 007 | loss: 0.07455 - acc: 0.9525 | val_loss: 0.59371 - val_acc: 0.8371 -- iter: 47616/55154\n",
      "--\n",
      "Training Step: 2959  | total loss: \u001b[1m\u001b[32m0.06980\u001b[0m\u001b[0m | time: 14572.331s\n",
      "| Adam | epoch: 007 | loss: 0.06980 - acc: 0.9534 | val_loss: 0.57073 - val_acc: 0.8414 -- iter: 47744/55154\n",
      "--\n",
      "Training Step: 2960  | total loss: \u001b[1m\u001b[32m0.07646\u001b[0m\u001b[0m | time: 14609.905s\n",
      "| Adam | epoch: 007 | loss: 0.07646 - acc: 0.9534 | val_loss: 0.55414 - val_acc: 0.8439 -- iter: 47872/55154\n",
      "--\n",
      "Training Step: 2961  | total loss: \u001b[1m\u001b[32m0.07558\u001b[0m\u001b[0m | time: 14652.953s\n",
      "| Adam | epoch: 007 | loss: 0.07558 - acc: 0.9541 | val_loss: 0.56285 - val_acc: 0.8403 -- iter: 48000/55154\n",
      "--\n",
      "Training Step: 2962  | total loss: \u001b[1m\u001b[32m0.08217\u001b[0m\u001b[0m | time: 14693.082s\n",
      "| Adam | epoch: 007 | loss: 0.08217 - acc: 0.9540 | val_loss: 0.59074 - val_acc: 0.8362 -- iter: 48128/55154\n",
      "--\n",
      "Training Step: 2963  | total loss: \u001b[1m\u001b[32m0.08957\u001b[0m\u001b[0m | time: 14730.893s\n",
      "| Adam | epoch: 007 | loss: 0.08957 - acc: 0.9516 | val_loss: 0.60164 - val_acc: 0.8332 -- iter: 48256/55154\n",
      "--\n",
      "Training Step: 2964  | total loss: \u001b[1m\u001b[32m0.08727\u001b[0m\u001b[0m | time: 14768.159s\n",
      "| Adam | epoch: 007 | loss: 0.08727 - acc: 0.9525 | val_loss: 0.56505 - val_acc: 0.8406 -- iter: 48384/55154\n",
      "--\n",
      "Training Step: 2965  | total loss: \u001b[1m\u001b[32m0.09923\u001b[0m\u001b[0m | time: 14805.712s\n",
      "| Adam | epoch: 007 | loss: 0.09923 - acc: 0.9495 | val_loss: 0.51785 - val_acc: 0.8517 -- iter: 48512/55154\n",
      "--\n",
      "Training Step: 2966  | total loss: \u001b[1m\u001b[32m0.09825\u001b[0m\u001b[0m | time: 14843.261s\n",
      "| Adam | epoch: 007 | loss: 0.09825 - acc: 0.9490 | val_loss: 0.51051 - val_acc: 0.8519 -- iter: 48640/55154\n",
      "--\n",
      "Training Step: 2967  | total loss: \u001b[1m\u001b[32m0.09007\u001b[0m\u001b[0m | time: 14880.481s\n",
      "| Adam | epoch: 007 | loss: 0.09007 - acc: 0.9494 | val_loss: 0.50292 - val_acc: 0.8509 -- iter: 48768/55154\n",
      "--\n",
      "Training Step: 2968  | total loss: \u001b[1m\u001b[32m0.08781\u001b[0m\u001b[0m | time: 14918.441s\n",
      "| Adam | epoch: 007 | loss: 0.08781 - acc: 0.9490 | val_loss: 0.50082 - val_acc: 0.8501 -- iter: 48896/55154\n",
      "--\n",
      "Training Step: 2969  | total loss: \u001b[1m\u001b[32m0.08360\u001b[0m\u001b[0m | time: 14956.307s\n",
      "| Adam | epoch: 007 | loss: 0.08360 - acc: 0.9510 | val_loss: 0.50555 - val_acc: 0.8492 -- iter: 49024/55154\n",
      "--\n",
      "Training Step: 2970  | total loss: \u001b[1m\u001b[32m0.08052\u001b[0m\u001b[0m | time: 14997.915s\n",
      "| Adam | epoch: 007 | loss: 0.08052 - acc: 0.9512 | val_loss: 0.50421 - val_acc: 0.8512 -- iter: 49152/55154\n",
      "--\n",
      "Training Step: 2971  | total loss: \u001b[1m\u001b[32m0.08195\u001b[0m\u001b[0m | time: 15035.593s\n",
      "| Adam | epoch: 007 | loss: 0.08195 - acc: 0.9506 | val_loss: 0.52950 - val_acc: 0.8475 -- iter: 49280/55154\n",
      "--\n",
      "Training Step: 2972  | total loss: \u001b[1m\u001b[32m0.07961\u001b[0m\u001b[0m | time: 15073.012s\n",
      "| Adam | epoch: 007 | loss: 0.07961 - acc: 0.9501 | val_loss: 0.56778 - val_acc: 0.8418 -- iter: 49408/55154\n",
      "--\n",
      "Training Step: 2973  | total loss: \u001b[1m\u001b[32m0.07324\u001b[0m\u001b[0m | time: 15111.020s\n",
      "| Adam | epoch: 007 | loss: 0.07324 - acc: 0.9535 | val_loss: 0.61942 - val_acc: 0.8325 -- iter: 49536/55154\n",
      "--\n",
      "Training Step: 2974  | total loss: \u001b[1m\u001b[32m0.07177\u001b[0m\u001b[0m | time: 15149.052s\n",
      "| Adam | epoch: 007 | loss: 0.07177 - acc: 0.9543 | val_loss: 0.59009 - val_acc: 0.8373 -- iter: 49664/55154\n",
      "--\n",
      "Training Step: 2975  | total loss: \u001b[1m\u001b[32m0.08495\u001b[0m\u001b[0m | time: 15186.833s\n",
      "| Adam | epoch: 007 | loss: 0.08495 - acc: 0.9510 | val_loss: 0.53064 - val_acc: 0.8468 -- iter: 49792/55154\n",
      "--\n",
      "Training Step: 2976  | total loss: \u001b[1m\u001b[32m0.08466\u001b[0m\u001b[0m | time: 15224.986s\n",
      "| Adam | epoch: 007 | loss: 0.08466 - acc: 0.9528 | val_loss: 0.49310 - val_acc: 0.8534 -- iter: 49920/55154\n",
      "--\n",
      "Training Step: 2977  | total loss: \u001b[1m\u001b[32m0.08242\u001b[0m\u001b[0m | time: 15263.205s\n",
      "| Adam | epoch: 007 | loss: 0.08242 - acc: 0.9513 | val_loss: 0.49815 - val_acc: 0.8506 -- iter: 50048/55154\n",
      "--\n",
      "Training Step: 2978  | total loss: \u001b[1m\u001b[32m0.08644\u001b[0m\u001b[0m | time: 15300.797s\n",
      "| Adam | epoch: 007 | loss: 0.08644 - acc: 0.9507 | val_loss: 0.53016 - val_acc: 0.8425 -- iter: 50176/55154\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2979  | total loss: \u001b[1m\u001b[32m0.09083\u001b[0m\u001b[0m | time: 15337.900s\n",
      "| Adam | epoch: 007 | loss: 0.09083 - acc: 0.9494 | val_loss: 0.55296 - val_acc: 0.8378 -- iter: 50304/55154\n",
      "--\n",
      "Training Step: 2980  | total loss: \u001b[1m\u001b[32m0.09516\u001b[0m\u001b[0m | time: 15375.190s\n",
      "| Adam | epoch: 007 | loss: 0.09516 - acc: 0.9497 | val_loss: 0.54881 - val_acc: 0.8387 -- iter: 50432/55154\n",
      "--\n",
      "Training Step: 2981  | total loss: \u001b[1m\u001b[32m0.09485\u001b[0m\u001b[0m | time: 15412.760s\n",
      "| Adam | epoch: 007 | loss: 0.09485 - acc: 0.9485 | val_loss: 0.52036 - val_acc: 0.8471 -- iter: 50560/55154\n",
      "--\n",
      "Training Step: 2982  | total loss: \u001b[1m\u001b[32m0.09626\u001b[0m\u001b[0m | time: 15450.363s\n",
      "| Adam | epoch: 007 | loss: 0.09626 - acc: 0.9505 | val_loss: 0.52283 - val_acc: 0.8470 -- iter: 50688/55154\n",
      "--\n",
      "Training Step: 2983  | total loss: \u001b[1m\u001b[32m0.09437\u001b[0m\u001b[0m | time: 15487.923s\n",
      "| Adam | epoch: 007 | loss: 0.09437 - acc: 0.9524 | val_loss: 0.52387 - val_acc: 0.8462 -- iter: 50816/55154\n",
      "--\n",
      "Training Step: 2984  | total loss: \u001b[1m\u001b[32m0.09611\u001b[0m\u001b[0m | time: 15525.120s\n",
      "| Adam | epoch: 007 | loss: 0.09611 - acc: 0.9548 | val_loss: 0.52113 - val_acc: 0.8456 -- iter: 50944/55154\n",
      "--\n",
      "Training Step: 2985  | total loss: \u001b[1m\u001b[32m0.10298\u001b[0m\u001b[0m | time: 15564.632s\n",
      "| Adam | epoch: 007 | loss: 0.10298 - acc: 0.9523 | val_loss: 0.55102 - val_acc: 0.8373 -- iter: 51072/55154\n",
      "--\n",
      "Training Step: 2986  | total loss: \u001b[1m\u001b[32m1.95805\u001b[0m\u001b[0m | time: 15601.564s\n",
      "| Adam | epoch: 007 | loss: 1.95805 - acc: 0.8594 | val_loss: 0.55281 - val_acc: 0.8362 -- iter: 51200/55154\n",
      "--\n",
      "Training Step: 2987  | total loss: \u001b[1m\u001b[32m1.77002\u001b[0m\u001b[0m | time: 15638.811s\n",
      "| Adam | epoch: 007 | loss: 1.77002 - acc: 0.8695 | val_loss: 0.55473 - val_acc: 0.8347 -- iter: 51328/55154\n",
      "--\n",
      "Training Step: 2988  | total loss: \u001b[1m\u001b[32m1.59689\u001b[0m\u001b[0m | time: 15676.113s\n",
      "| Adam | epoch: 007 | loss: 1.59689 - acc: 0.8787 | val_loss: 0.50179 - val_acc: 0.8426 -- iter: 51456/55154\n",
      "--\n",
      "Training Step: 2989  | total loss: \u001b[1m\u001b[32m1.44783\u001b[0m\u001b[0m | time: 15712.686s\n",
      "| Adam | epoch: 007 | loss: 1.44783 - acc: 0.8838 | val_loss: 0.45031 - val_acc: 0.8522 -- iter: 51584/55154\n",
      "--\n",
      "Training Step: 2990  | total loss: \u001b[1m\u001b[32m1.31484\u001b[0m\u001b[0m | time: 15749.832s\n",
      "| Adam | epoch: 007 | loss: 1.31484 - acc: 0.8899 | val_loss: 0.42833 - val_acc: 0.8554 -- iter: 51712/55154\n",
      "--\n",
      "Training Step: 2991  | total loss: \u001b[1m\u001b[32m1.18829\u001b[0m\u001b[0m | time: 15786.396s\n",
      "| Adam | epoch: 007 | loss: 1.18829 - acc: 0.8986 | val_loss: 0.42755 - val_acc: 0.8544 -- iter: 51840/55154\n",
      "--\n",
      "Training Step: 2992  | total loss: \u001b[1m\u001b[32m1.07555\u001b[0m\u001b[0m | time: 15823.415s\n",
      "| Adam | epoch: 007 | loss: 1.07555 - acc: 0.9033 | val_loss: 0.42776 - val_acc: 0.8538 -- iter: 51968/55154\n",
      "--\n",
      "Training Step: 2993  | total loss: \u001b[1m\u001b[32m0.97829\u001b[0m\u001b[0m | time: 15861.619s\n",
      "| Adam | epoch: 007 | loss: 0.97829 - acc: 0.9051 | val_loss: 0.41462 - val_acc: 0.8587 -- iter: 52096/55154\n",
      "--\n",
      "Training Step: 2994  | total loss: \u001b[1m\u001b[32m0.89870\u001b[0m\u001b[0m | time: 15899.157s\n",
      "| Adam | epoch: 007 | loss: 0.89870 - acc: 0.9091 | val_loss: 0.42488 - val_acc: 0.8563 -- iter: 52224/55154\n",
      "--\n",
      "Training Step: 2995  | total loss: \u001b[1m\u001b[32m0.81477\u001b[0m\u001b[0m | time: 15937.515s\n",
      "| Adam | epoch: 007 | loss: 0.81477 - acc: 0.9159 | val_loss: 0.45101 - val_acc: 0.8496 -- iter: 52352/55154\n",
      "--\n",
      "Training Step: 2996  | total loss: \u001b[1m\u001b[32m0.74060\u001b[0m\u001b[0m | time: 15975.749s\n",
      "| Adam | epoch: 007 | loss: 0.74060 - acc: 0.9227 | val_loss: 0.47029 - val_acc: 0.8464 -- iter: 52480/55154\n",
      "--\n",
      "Training Step: 2997  | total loss: \u001b[1m\u001b[32m0.67458\u001b[0m\u001b[0m | time: 16018.978s\n",
      "| Adam | epoch: 007 | loss: 0.67458 - acc: 0.9281 | val_loss: 0.47274 - val_acc: 0.8465 -- iter: 52608/55154\n",
      "--\n",
      "Training Step: 2998  | total loss: \u001b[1m\u001b[32m0.61676\u001b[0m\u001b[0m | time: 16058.010s\n",
      "| Adam | epoch: 007 | loss: 0.61676 - acc: 0.9291 | val_loss: 0.46996 - val_acc: 0.8480 -- iter: 52736/55154\n",
      "--\n",
      "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.55945\u001b[0m\u001b[0m | time: 16095.143s\n",
      "| Adam | epoch: 007 | loss: 0.55945 - acc: 0.9338 | val_loss: 0.46142 - val_acc: 0.8508 -- iter: 52864/55154\n",
      "--\n",
      "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.50918\u001b[0m\u001b[0m | time: 16132.413s\n",
      "| Adam | epoch: 007 | loss: 0.50918 - acc: 0.9342 | val_loss: 0.46117 - val_acc: 0.8505 -- iter: 52992/55154\n",
      "--\n",
      "Training Step: 3001  | total loss: \u001b[1m\u001b[32m0.46070\u001b[0m\u001b[0m | time: 16172.691s\n",
      "| Adam | epoch: 007 | loss: 0.46070 - acc: 0.9384 | val_loss: 0.45356 - val_acc: 0.8536 -- iter: 53120/55154\n",
      "--\n",
      "Training Step: 3002  | total loss: \u001b[1m\u001b[32m0.42085\u001b[0m\u001b[0m | time: 16212.453s\n",
      "| Adam | epoch: 007 | loss: 0.42085 - acc: 0.9399 | val_loss: 0.44652 - val_acc: 0.8553 -- iter: 53248/55154\n",
      "--\n",
      "Training Step: 3003  | total loss: \u001b[1m\u001b[32m0.38756\u001b[0m\u001b[0m | time: 16249.811s\n",
      "| Adam | epoch: 007 | loss: 0.38756 - acc: 0.9436 | val_loss: 0.44058 - val_acc: 0.8573 -- iter: 53376/55154\n",
      "--\n",
      "Training Step: 3004  | total loss: \u001b[1m\u001b[32m0.35161\u001b[0m\u001b[0m | time: 16287.134s\n",
      "| Adam | epoch: 007 | loss: 0.35161 - acc: 0.9469 | val_loss: 0.43592 - val_acc: 0.8583 -- iter: 53504/55154\n",
      "--\n",
      "Training Step: 3005  | total loss: \u001b[1m\u001b[32m0.31824\u001b[0m\u001b[0m | time: 16325.886s\n",
      "| Adam | epoch: 007 | loss: 0.31824 - acc: 0.9522 | val_loss: 0.43492 - val_acc: 0.8582 -- iter: 53632/55154\n",
      "--\n",
      "Training Step: 3006  | total loss: \u001b[1m\u001b[32m0.28747\u001b[0m\u001b[0m | time: 16383.051s\n",
      "| Adam | epoch: 007 | loss: 0.28747 - acc: 0.9546 | val_loss: 0.43262 - val_acc: 0.8577 -- iter: 53760/55154\n",
      "--\n",
      "Training Step: 3007  | total loss: \u001b[1m\u001b[32m0.26316\u001b[0m\u001b[0m | time: 16442.306s\n",
      "| Adam | epoch: 007 | loss: 0.26316 - acc: 0.9552 | val_loss: 0.43330 - val_acc: 0.8573 -- iter: 53888/55154\n",
      "--\n",
      "Training Step: 3008  | total loss: \u001b[1m\u001b[32m0.23824\u001b[0m\u001b[0m | time: 16501.783s\n",
      "| Adam | epoch: 007 | loss: 0.23824 - acc: 0.9566 | val_loss: 0.43587 - val_acc: 0.8563 -- iter: 54016/55154\n",
      "--\n",
      "Training Step: 3009  | total loss: \u001b[1m\u001b[32m0.21971\u001b[0m\u001b[0m | time: 16560.926s\n",
      "| Adam | epoch: 007 | loss: 0.21971 - acc: 0.9578 | val_loss: 0.44014 - val_acc: 0.8553 -- iter: 54144/55154\n",
      "--\n",
      "Training Step: 3010  | total loss: \u001b[1m\u001b[32m0.20212\u001b[0m\u001b[0m | time: 16619.542s\n",
      "| Adam | epoch: 007 | loss: 0.20212 - acc: 0.9605 | val_loss: 0.43766 - val_acc: 0.8578 -- iter: 54272/55154\n",
      "--\n",
      "Training Step: 3011  | total loss: \u001b[1m\u001b[32m0.19372\u001b[0m\u001b[0m | time: 16677.933s\n",
      "| Adam | epoch: 007 | loss: 0.19372 - acc: 0.9574 | val_loss: 0.43386 - val_acc: 0.8596 -- iter: 54400/55154\n",
      "--\n",
      "Training Step: 3012  | total loss: \u001b[1m\u001b[32m0.18390\u001b[0m\u001b[0m | time: 16736.138s\n",
      "| Adam | epoch: 007 | loss: 0.18390 - acc: 0.9585 | val_loss: 0.43461 - val_acc: 0.8606 -- iter: 54528/55154\n",
      "--\n",
      "Training Step: 3013  | total loss: \u001b[1m\u001b[32m0.16864\u001b[0m\u001b[0m | time: 16794.723s\n",
      "| Adam | epoch: 007 | loss: 0.16864 - acc: 0.9588 | val_loss: 0.44440 - val_acc: 0.8594 -- iter: 54656/55154\n",
      "--\n",
      "Training Step: 3014  | total loss: \u001b[1m\u001b[32m0.16264\u001b[0m\u001b[0m | time: 16851.935s\n",
      "| Adam | epoch: 007 | loss: 0.16264 - acc: 0.9574 | val_loss: 0.45392 - val_acc: 0.8584 -- iter: 54784/55154\n",
      "--\n",
      "Training Step: 3015  | total loss: \u001b[1m\u001b[32m0.15363\u001b[0m\u001b[0m | time: 16909.655s\n",
      "| Adam | epoch: 007 | loss: 0.15363 - acc: 0.9570 | val_loss: 0.46138 - val_acc: 0.8568 -- iter: 54912/55154\n",
      "--\n",
      "Training Step: 3016  | total loss: \u001b[1m\u001b[32m0.14421\u001b[0m\u001b[0m | time: 16968.149s\n",
      "| Adam | epoch: 007 | loss: 0.14421 - acc: 0.9558 | val_loss: 0.46583 - val_acc: 0.8547 -- iter: 55040/55154\n",
      "--\n",
      "Training Step: 3017  | total loss: \u001b[1m\u001b[32m0.13581\u001b[0m\u001b[0m | time: 17026.596s\n",
      "| Adam | epoch: 007 | loss: 0.13581 - acc: 0.9556 | val_loss: 0.47852 - val_acc: 0.8522 -- iter: 55154/55154\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit({'input': X}, {'targets': Y}, n_epoch = 1,validation_set=({'input': test_x}, {'targets': test_y}), batch_size= 128, snapshot_step =1, show_metric= True, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:C:\\Users\\hiremath\\Downloads\\devanagari-0.001-5conv-basic.model is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_DIR = 'C:\\\\Users\\\\hiremath\\\\Downloads\\\\devanagari\\\\Images\\\\test1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_test_data():\n",
    "    test_data = []\n",
    "    for img in os.listdir(TEST_DIR):\n",
    "        if img != \"Thumbs - Copy.db\" and img != \"Thumbs.db\":\n",
    "            path1 = os.path.join(TEST_DIR,img)\n",
    "            with Image.open(path1).convert('L') as img1:\n",
    "                img1 = img1.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n",
    "                test_data.append([np.array(img1)])\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,v):\n",
    "    scores = model.predict(X)[i]\n",
    "    val = np.argmax(scores)\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
